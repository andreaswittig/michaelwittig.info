[{"title":"Amazon Web Services in Action","description":"<p><img src=\"http://blog.michaelwittig.info/content/images/2015/04/wittig_cover150.jpg\" alt=\"Amazon Web Services in Action\"></p>\n\n<p><a href=\"http://andreaswittig.info/\">Andreas</a> und ich schreiben derzeit an einem Buch über Amazon Web Services. Unser Ziel ist es eine umfangreiche Einführung in die Welt von AWS zu geben. Fokus liegt dabei auf der Automatisierung von IT-Infrastruktur.</p>\n\n<p>Ab sofort ist die Early Access Edition von <a href=\"http://manning.com/wittig?a_aid=mwittig&amp;a_bid=cc17df85\">Amazon Web Services in Action</a> verfügbar. Man kann das Buch also schon lesen, bevor es fertig ist. Verrückte agile Welt!</p>\n\n<p>Mit dem Gutschein-Code mlwittig gibt es 50% Rabatt: <a href=\"http://manning.com/wittig?a_aid=mwittig&amp;a_bid=cc17df85\">Amazon Web Services in Action</a></p>\n\n<p>Blog oder lieber Buch? Beides. Wir freuen uns sehr über Feedback aus unserer Blog-Leserschaft freuen!</p>","summary":"<p><img src=\"http://blog.michaelwittig.info/content/images/2015/04/wittig_cover150.jpg\" alt=\"Amazon Web Services in Action\"></p>\n\n<p><a href=\"http://andreaswittig.info/\">Andreas</a> und ich schreiben derzeit an einem Buch über Amazon Web Services. Unser Ziel ist es eine umfangreiche Einführung in die Welt von AWS zu geben. Fokus liegt dabei auf der Automatisierung von IT-Infrastruktur.</p>\n\n<p>Ab sofort ist die Early Access Edition von <a href=\"http://manning.com/wittig?a_aid=mwittig&amp;a_bid=cc17df85\">Amazon Web Services in Action</a> verfügbar. Man kann das Buch also schon lesen, bevor es fertig ist. Verrückte agile Welt!</p>\n\n<p>Mit dem Gutschein-Code mlwittig gibt es 50% Rabatt: <a href=\"http://manning.com/wittig?a_aid=mwittig&amp;a_bid=cc17df85\">Amazon Web Services in Action</a></p>\n\n<p>Blog oder lieber Buch? Beides. Wir freuen uns sehr über Feedback aus unserer Blog-Leserschaft freuen!</p>","date":"2015-04-27T17:17:48.000Z","pubdate":"2015-04-27T17:17:48.000Z","pubDate":"2015-04-27T17:17:48.000Z","link":"http://blog.michaelwittig.info/amazon-web-serv/","guid":"acd3547c-1101-4a4c-b8b6-57564a469303","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["AWS","AWSinAction"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Amazon Web Services in Action"},"rss:description":{"@":{},"#":"<p><img src=\"http://blog.michaelwittig.info/content/images/2015/04/wittig_cover150.jpg\" alt=\"Amazon Web Services in Action\"></p>\n\n<p><a href=\"http://andreaswittig.info/\">Andreas</a> und ich schreiben derzeit an einem Buch über Amazon Web Services. Unser Ziel ist es eine umfangreiche Einführung in die Welt von AWS zu geben. Fokus liegt dabei auf der Automatisierung von IT-Infrastruktur.</p>\n\n<p>Ab sofort ist die Early Access Edition von <a href=\"http://manning.com/wittig?a_aid=mwittig&amp;a_bid=cc17df85\">Amazon Web Services in Action</a> verfügbar. Man kann das Buch also schon lesen, bevor es fertig ist. Verrückte agile Welt!</p>\n\n<p>Mit dem Gutschein-Code mlwittig gibt es 50% Rabatt: <a href=\"http://manning.com/wittig?a_aid=mwittig&amp;a_bid=cc17df85\">Amazon Web Services in Action</a></p>\n\n<p>Blog oder lieber Buch? Beides. Wir freuen uns sehr über Feedback aus unserer Blog-Leserschaft freuen!</p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/amazon-web-serv/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"acd3547c-1101-4a4c-b8b6-57564a469303"},"rss:category":[{"@":{},"#":"AWS"},{"@":{},"#":"AWSinAction"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Mon, 27 Apr 2015 17:17:48 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"DevOps Conference: Agile Banking mit DevOps und AWS","description":"<p>Innovation in der Finanzbranche ist technologiegetrieben und erfordert eine Time-To-Market von Stunden und nicht - wie bisher üblich - Jahren. DevOps und Cloud Computing bilden das Fundament, auf dem eine hohe Entwicklungsgeschwindigkeit bei gleichzeitig hoher Qualität und Sicherheit realisiert werden kann. Die Zauberworte dabei heißen automatischer Integrationstest, Infrastructure as Code und agile Testumgebungen. Dieser Vortrag beschreibt praxisnah die Transformation einer Wertpapierhandelsbank von On-Premise nach AWS (Amazon Web Services) und geht dabei auf technische Herausforderungen und den Wandel zu einer DevOps Kultur ein.</p>\n\n<p>Zu diesem Thema halte ich einen <a href=\"http://devopsconference.de/2015/de/sessions/agile-banking-mit-devops-und-aws\">Vortrag auf der DevOps Conference im Juni in Berlin</a>.</p>\n\n<p>Vorab gibt es schon ein <a href=\"https://jaxenter.de/devops-kreative-zerstoerung-17266\">kurzes Interview zum Vortrag</a>.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2015/04/DOC_Viewport_28496_v1.png\" alt=\"DevOps Conference\"></p>\n\n<h1 id=\"devopsconference\">DevOps Conference</h1>\n\n<p><a href=\"http://blog.michaelwittig.info/www.devopsconference.de\">DevOps Conference 2015</a> – Die Konferenz für Docker, Infrastructure as Code, Continuous Delivery, Cloud und Lean Business\nVom 1. bis 3. Juni 2015 präsentiert das Entwickler Magazin die neue DevOps Conference in Berlin. Die Konferenz richtet sich an DevOps-Entwickler, Administratoren, Software- und Systemarchitekten. Der Weg hin zu einem konsequenten DevOps-Ansatz in Unternehmen erfordert sowohl organisatorische Veränderungen als auch profundes technisches Know-how  - beides will die neue DevOpsCon umfassend vermitteln. Die DevOps Con bietet 30 Sessions und  Workshops, unterteilt in die drei parallelen Tracks Container, Tools &amp; Process und DevOps Business Day, sowie fast 30 nationale und internationale Sprecher.</p>","summary":"<p>Innovation in der Finanzbranche ist technologiegetrieben und erfordert eine Time-To-Market von Stunden und nicht - wie bisher üblich - Jahren. DevOps und Cloud Computing bilden das Fundament, auf dem eine hohe Entwicklungsgeschwindigkeit bei gleichzeitig hoher Qualität und Sicherheit realisiert werden kann. Die Zauberworte dabei heißen automatischer Integrationstest, Infrastructure as Code und agile Testumgebungen. Dieser Vortrag beschreibt praxisnah die Transformation einer Wertpapierhandelsbank von On-Premise nach AWS (Amazon Web Services) und geht dabei auf technische Herausforderungen und den Wandel zu einer DevOps Kultur ein.</p>\n\n<p>Zu diesem Thema halte ich einen <a href=\"http://devopsconference.de/2015/de/sessions/agile-banking-mit-devops-und-aws\">Vortrag auf der DevOps Conference im Juni in Berlin</a>.</p>\n\n<p>Vorab gibt es schon ein <a href=\"https://jaxenter.de/devops-kreative-zerstoerung-17266\">kurzes Interview zum Vortrag</a>.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2015/04/DOC_Viewport_28496_v1.png\" alt=\"DevOps Conference\"></p>\n\n<h1 id=\"devopsconference\">DevOps Conference</h1>\n\n<p><a href=\"http://blog.michaelwittig.info/www.devopsconference.de\">DevOps Conference 2015</a> – Die Konferenz für Docker, Infrastructure as Code, Continuous Delivery, Cloud und Lean Business\nVom 1. bis 3. Juni 2015 präsentiert das Entwickler Magazin die neue DevOps Conference in Berlin. Die Konferenz richtet sich an DevOps-Entwickler, Administratoren, Software- und Systemarchitekten. Der Weg hin zu einem konsequenten DevOps-Ansatz in Unternehmen erfordert sowohl organisatorische Veränderungen als auch profundes technisches Know-how  - beides will die neue DevOpsCon umfassend vermitteln. Die DevOps Con bietet 30 Sessions und  Workshops, unterteilt in die drei parallelen Tracks Container, Tools &amp; Process und DevOps Business Day, sowie fast 30 nationale und internationale Sprecher.</p>","date":"2015-04-27T08:32:40.000Z","pubdate":"2015-04-27T08:32:40.000Z","pubDate":"2015-04-27T08:32:40.000Z","link":"http://blog.michaelwittig.info/devops-conference/","guid":"9b22dabf-2074-4ae9-acbf-b66f5c780efe","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["Vortrag","DevOps"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"DevOps Conference: Agile Banking mit DevOps und AWS"},"rss:description":{"@":{},"#":"<p>Innovation in der Finanzbranche ist technologiegetrieben und erfordert eine Time-To-Market von Stunden und nicht - wie bisher üblich - Jahren. DevOps und Cloud Computing bilden das Fundament, auf dem eine hohe Entwicklungsgeschwindigkeit bei gleichzeitig hoher Qualität und Sicherheit realisiert werden kann. Die Zauberworte dabei heißen automatischer Integrationstest, Infrastructure as Code und agile Testumgebungen. Dieser Vortrag beschreibt praxisnah die Transformation einer Wertpapierhandelsbank von On-Premise nach AWS (Amazon Web Services) und geht dabei auf technische Herausforderungen und den Wandel zu einer DevOps Kultur ein.</p>\n\n<p>Zu diesem Thema halte ich einen <a href=\"http://devopsconference.de/2015/de/sessions/agile-banking-mit-devops-und-aws\">Vortrag auf der DevOps Conference im Juni in Berlin</a>.</p>\n\n<p>Vorab gibt es schon ein <a href=\"https://jaxenter.de/devops-kreative-zerstoerung-17266\">kurzes Interview zum Vortrag</a>.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2015/04/DOC_Viewport_28496_v1.png\" alt=\"DevOps Conference\"></p>\n\n<h1 id=\"devopsconference\">DevOps Conference</h1>\n\n<p><a href=\"http://blog.michaelwittig.info/www.devopsconference.de\">DevOps Conference 2015</a> – Die Konferenz für Docker, Infrastructure as Code, Continuous Delivery, Cloud und Lean Business\nVom 1. bis 3. Juni 2015 präsentiert das Entwickler Magazin die neue DevOps Conference in Berlin. Die Konferenz richtet sich an DevOps-Entwickler, Administratoren, Software- und Systemarchitekten. Der Weg hin zu einem konsequenten DevOps-Ansatz in Unternehmen erfordert sowohl organisatorische Veränderungen als auch profundes technisches Know-how  - beides will die neue DevOpsCon umfassend vermitteln. Die DevOps Con bietet 30 Sessions und  Workshops, unterteilt in die drei parallelen Tracks Container, Tools &amp; Process und DevOps Business Day, sowie fast 30 nationale und internationale Sprecher.</p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/devops-conference/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"9b22dabf-2074-4ae9-acbf-b66f5c780efe"},"rss:category":[{"@":{},"#":"Vortrag"},{"@":{},"#":"DevOps"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Mon, 27 Apr 2015 08:32:40 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Datenbank am Limit: spaltenorientiert als Ausweg","description":"<p>Wir alle kennen und schätzen SQL- und NoSQL-Datenbanken. Doch es gibt Anwendungsfälle, in denen diese Datenbanken an ihre Grenzen stoßen. Zum Beispiel bei der Analyse von Finanzmarktdaten. Dort müssen Zeitreihen von enormer Größe verarbeitet werden.</p>\n\n<p>Am 22. April 2015 habe ich dazu einen <a href=\"http://bigdatacon.de/2015/sessions/datenbank-am-limit-spaltenorientiert-als-ausweg\">Vortrag auf der BigDataCon</a> in Mainz gehalten.</p>\n\n<p>Der Vortrag zeigt auf, wie spaltenorientierte Datenbanken dieses Problem lösen. Die Architektur solcher Tick-Data-Systeme wird beleuchtet. Der Vortrag endet mit dem Beispiel einer technischen Implementierung für Finanzmarktdaten.</p>\n\n<p>Folien zum Vortrag gibt es bei <a href=\"https://speakerdeck.com/michaelwittig/datenbank-am-limit-spaltenorientiert-als-ausweg\">Speacker Deck</a>.</p>","summary":"<p>Wir alle kennen und schätzen SQL- und NoSQL-Datenbanken. Doch es gibt Anwendungsfälle, in denen diese Datenbanken an ihre Grenzen stoßen. Zum Beispiel bei der Analyse von Finanzmarktdaten. Dort müssen Zeitreihen von enormer Größe verarbeitet werden.</p>\n\n<p>Am 22. April 2015 habe ich dazu einen <a href=\"http://bigdatacon.de/2015/sessions/datenbank-am-limit-spaltenorientiert-als-ausweg\">Vortrag auf der BigDataCon</a> in Mainz gehalten.</p>\n\n<p>Der Vortrag zeigt auf, wie spaltenorientierte Datenbanken dieses Problem lösen. Die Architektur solcher Tick-Data-Systeme wird beleuchtet. Der Vortrag endet mit dem Beispiel einer technischen Implementierung für Finanzmarktdaten.</p>\n\n<p>Folien zum Vortrag gibt es bei <a href=\"https://speakerdeck.com/michaelwittig/datenbank-am-limit-spaltenorientiert-als-ausweg\">Speacker Deck</a>.</p>","date":"2015-04-23T13:10:50.000Z","pubdate":"2015-04-23T13:10:50.000Z","pubDate":"2015-04-23T13:10:50.000Z","link":"http://blog.michaelwittig.info/datenbank-am-limit-spaltenorientiert-als-ausweg/","guid":"7c12b144-1e9b-4591-b923-236d2abac670","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["kdb+","BigData","kdb+tick","Vortrag","MonetDB","MongoDB"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Datenbank am Limit: spaltenorientiert als Ausweg"},"rss:description":{"@":{},"#":"<p>Wir alle kennen und schätzen SQL- und NoSQL-Datenbanken. Doch es gibt Anwendungsfälle, in denen diese Datenbanken an ihre Grenzen stoßen. Zum Beispiel bei der Analyse von Finanzmarktdaten. Dort müssen Zeitreihen von enormer Größe verarbeitet werden.</p>\n\n<p>Am 22. April 2015 habe ich dazu einen <a href=\"http://bigdatacon.de/2015/sessions/datenbank-am-limit-spaltenorientiert-als-ausweg\">Vortrag auf der BigDataCon</a> in Mainz gehalten.</p>\n\n<p>Der Vortrag zeigt auf, wie spaltenorientierte Datenbanken dieses Problem lösen. Die Architektur solcher Tick-Data-Systeme wird beleuchtet. Der Vortrag endet mit dem Beispiel einer technischen Implementierung für Finanzmarktdaten.</p>\n\n<p>Folien zum Vortrag gibt es bei <a href=\"https://speakerdeck.com/michaelwittig/datenbank-am-limit-spaltenorientiert-als-ausweg\">Speacker Deck</a>.</p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/datenbank-am-limit-spaltenorientiert-als-ausweg/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"7c12b144-1e9b-4591-b923-236d2abac670"},"rss:category":[{"@":{},"#":"kdb+"},{"@":{},"#":"BigData"},{"@":{},"#":"kdb+tick"},{"@":{},"#":"Vortrag"},{"@":{},"#":"MonetDB"},{"@":{},"#":"MongoDB"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Thu, 23 Apr 2015 13:10:50 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Spaltenorientierte Datenbank: Konzept (Teil 2)","description":"<p>Im 2. Teil wollen wir uns detaillierter mit dem Konzept einer spaltenorientierte Datenbank beschäftigen. Als Beispiel entwickeln wir das Konzept für eine Time Series Database (TSDB). Im <a href=\"http://blog.michaelwittig.info/spaltenorientierte-datenbank-intro-teil-1/\">Teil 1</a> der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> haben wir uns den Unterschied zwischen zeilenorientiert und spaltenorientiert angeschaut. </p>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-draft-part-2-21199a2de18a\">Read this article in english</a></p>\n</blockquote>\n\n<h2 id=\"ondiskkonzept\">On-Disk Konzept</h2>\n\n<p>Die Datenbank ist spaltenorientiert. Das bedeutet, dass jede Spalte als Datei repräsentiert wird. Die erste Spalte ist immer der Zeitstempel. Gefolgt von mehreren dazugehörigen Werten.</p>\n\n<pre><code>Zeitstempel     Wert1     Wert2\n====================================\n10:01:00.000    324       34654\n11:05:00.000    342       23463\n12:17:00.000    446       47232\n13:03:00.000    234       60383\n14:31:00.000    213       73947\n15:14:00.000    342       46286\n</code></pre>\n\n<p>Wichtig ist, dass die Einträge aufsteigend nach Zeitstempel sortiert sind. Werden neue Daten hinzugefügt müssen diese also nur ans Ende der Dateien angehängt werden und schon sind sie sortiert. Eine Weitere Einschränkung ist, dass Spalten nur gleiche Typen beinhalten dürfen. Für den Anfang wird der Zeitstempel als 32-bit unsigned Integer abgespeichert und alle Werte als 32-bit signed Float.</p>\n\n<p>Die wichtigste Abfrage wird eine Selektion auf die Zeitstempel sein. Da diese sortiert vorliegen können wir eine <a href=\"http://de.wikipedia.org/wiki/Binäre_Suche\">Binäre Suche</a> nutzen um effizient den Anfang der Zeitreihe in der Datei zu finden. Da Werte alle die gleiche Länge (32-bit) haben, kann auch in den Dateien der Werte an die richtige Position gesprungen werden.</p>\n\n<p>Damit auch sehr große Zeitreihen unterstützt werden können bietet sich eine Partitionierung an. Entweder eine fixe Partitionierung z. B. ein Tag oder auch eine bestimmte Anzahl an Elementen.</p>\n\n<pre><code>/\n    2014-01-01/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n    2014-01-02/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n</code></pre>\n\n<p>Oder eine variable Partitionierung .z B. gibt der Ordnername den ersten Zeitstempel an.</p>\n\n<pre><code>  /\n    10:01:00.000/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n    14:31:00.000/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n</code></pre>\n\n<h2 id=\"writekonzept\">Write Konzept</h2>\n\n<p>Da wir in mehrere Dateien gleichzeitig schreiben müssen (Zeitstempel.dat, Wert1.dat, Wert2.dat, ...) und diese  gleich viele Einträge haben müssen, muss zudem sichergestellt sein, dass während eines Schreibvorganges kein Lesezugriff erfolgen kann oder dieser immer defensiv erfolgt. So könnte zum Beispiel immer zuerst die Zeitstempel.dat gelesen werden. Geschrieben wird aber immer zuerst in die Wert2.dat. So kann es nie vorkommen, dass Zeitstempel existieren für die es noch keine Werte gibt. Diese Methode ist wesentlich effizienter, da <a href=\"http://de.wikipedia.org/wiki/Nicht-blockierende_Synchronisation\">keine künstliche Synchronisation</a> benötigt wird.</p>\n\n<h2 id=\"readkonzept\">Read Konzept</h2>\n\n<p>Um Daten möglichst schnell zu lesen müssen so wenig wie möglich Daten gelesen werden. Das gewählte On-Disk Konzept erfüllt diese Voraussetzung. Evtl. kann noch über eine Komprimierung nachgedacht werden, da das Entpacken bis zu einem gewissen Grad schneller ist als die Datenmenge von langsamen Festplatten zu lesen.</p>\n\n<p>Des weiteren muss noch darauf geachtet werden, dass niemals die gesamte Datenmenge in den Speicher passen muss. Sog. Streaming Algorithmen erfüllen diesen Zweck.</p>\n\n<p>Als Caching Mechanismus benutzen wir die vom Betriebsystem bereitgestellten Mechanismen für IO Caching. So werden Anfragen auf den gleichen Datenbestand deutlich schneller bearbeitet, da nicht von Festplatte gelesen werden muss.</p>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Durch die Verwendung von normalen Dateien und ein paar Tricks lässt sich eine gute Grundlage für eine Time Series Database legen. Durch die Ausnutzung der Möglichkeiten ist es möglich mit relativ wenig Aufwand ein großes Datenbanksystem zu betrieben.</p>\n\n<ul>\n<li>TimeSeries.Guru ist eine spaltenorientierte <a href=\"http://www.timeseries.guru/\">Time Series Database as a Service</a></li>\n<li>MonetDB ist eine <a href=\"https://www.monetdb.org/\">Open Source spaltenorientierte Datenbank</a></li>\n<li>Wikipedia Artikel zu <a href=\"http://de.wikipedia.org/wiki/Spaltenorientierte_Datenbank\">Spaltenorientierte Datenbank</a></li>\n</ul>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-draft-part-2-21199a2de18a\">Read this article in english</a></p>\n</blockquote>","summary":"<p>Im 2. Teil wollen wir uns detaillierter mit dem Konzept einer spaltenorientierte Datenbank beschäftigen. Als Beispiel entwickeln wir das Konzept für eine Time Series Database (TSDB). Im <a href=\"http://blog.michaelwittig.info/spaltenorientierte-datenbank-intro-teil-1/\">Teil 1</a> der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> haben wir uns den Unterschied zwischen zeilenorientiert und spaltenorientiert angeschaut. </p>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-draft-part-2-21199a2de18a\">Read this article in english</a></p>\n</blockquote>\n\n<h2 id=\"ondiskkonzept\">On-Disk Konzept</h2>\n\n<p>Die Datenbank ist spaltenorientiert. Das bedeutet, dass jede Spalte als Datei repräsentiert wird. Die erste Spalte ist immer der Zeitstempel. Gefolgt von mehreren dazugehörigen Werten.</p>\n\n<pre><code>Zeitstempel     Wert1     Wert2\n====================================\n10:01:00.000    324       34654\n11:05:00.000    342       23463\n12:17:00.000    446       47232\n13:03:00.000    234       60383\n14:31:00.000    213       73947\n15:14:00.000    342       46286\n</code></pre>\n\n<p>Wichtig ist, dass die Einträge aufsteigend nach Zeitstempel sortiert sind. Werden neue Daten hinzugefügt müssen diese also nur ans Ende der Dateien angehängt werden und schon sind sie sortiert. Eine Weitere Einschränkung ist, dass Spalten nur gleiche Typen beinhalten dürfen. Für den Anfang wird der Zeitstempel als 32-bit unsigned Integer abgespeichert und alle Werte als 32-bit signed Float.</p>\n\n<p>Die wichtigste Abfrage wird eine Selektion auf die Zeitstempel sein. Da diese sortiert vorliegen können wir eine <a href=\"http://de.wikipedia.org/wiki/Binäre_Suche\">Binäre Suche</a> nutzen um effizient den Anfang der Zeitreihe in der Datei zu finden. Da Werte alle die gleiche Länge (32-bit) haben, kann auch in den Dateien der Werte an die richtige Position gesprungen werden.</p>\n\n<p>Damit auch sehr große Zeitreihen unterstützt werden können bietet sich eine Partitionierung an. Entweder eine fixe Partitionierung z. B. ein Tag oder auch eine bestimmte Anzahl an Elementen.</p>\n\n<pre><code>/\n    2014-01-01/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n    2014-01-02/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n</code></pre>\n\n<p>Oder eine variable Partitionierung .z B. gibt der Ordnername den ersten Zeitstempel an.</p>\n\n<pre><code>  /\n    10:01:00.000/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n    14:31:00.000/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n</code></pre>\n\n<h2 id=\"writekonzept\">Write Konzept</h2>\n\n<p>Da wir in mehrere Dateien gleichzeitig schreiben müssen (Zeitstempel.dat, Wert1.dat, Wert2.dat, ...) und diese  gleich viele Einträge haben müssen, muss zudem sichergestellt sein, dass während eines Schreibvorganges kein Lesezugriff erfolgen kann oder dieser immer defensiv erfolgt. So könnte zum Beispiel immer zuerst die Zeitstempel.dat gelesen werden. Geschrieben wird aber immer zuerst in die Wert2.dat. So kann es nie vorkommen, dass Zeitstempel existieren für die es noch keine Werte gibt. Diese Methode ist wesentlich effizienter, da <a href=\"http://de.wikipedia.org/wiki/Nicht-blockierende_Synchronisation\">keine künstliche Synchronisation</a> benötigt wird.</p>\n\n<h2 id=\"readkonzept\">Read Konzept</h2>\n\n<p>Um Daten möglichst schnell zu lesen müssen so wenig wie möglich Daten gelesen werden. Das gewählte On-Disk Konzept erfüllt diese Voraussetzung. Evtl. kann noch über eine Komprimierung nachgedacht werden, da das Entpacken bis zu einem gewissen Grad schneller ist als die Datenmenge von langsamen Festplatten zu lesen.</p>\n\n<p>Des weiteren muss noch darauf geachtet werden, dass niemals die gesamte Datenmenge in den Speicher passen muss. Sog. Streaming Algorithmen erfüllen diesen Zweck.</p>\n\n<p>Als Caching Mechanismus benutzen wir die vom Betriebsystem bereitgestellten Mechanismen für IO Caching. So werden Anfragen auf den gleichen Datenbestand deutlich schneller bearbeitet, da nicht von Festplatte gelesen werden muss.</p>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Durch die Verwendung von normalen Dateien und ein paar Tricks lässt sich eine gute Grundlage für eine Time Series Database legen. Durch die Ausnutzung der Möglichkeiten ist es möglich mit relativ wenig Aufwand ein großes Datenbanksystem zu betrieben.</p>\n\n<ul>\n<li>TimeSeries.Guru ist eine spaltenorientierte <a href=\"http://www.timeseries.guru/\">Time Series Database as a Service</a></li>\n<li>MonetDB ist eine <a href=\"https://www.monetdb.org/\">Open Source spaltenorientierte Datenbank</a></li>\n<li>Wikipedia Artikel zu <a href=\"http://de.wikipedia.org/wiki/Spaltenorientierte_Datenbank\">Spaltenorientierte Datenbank</a></li>\n</ul>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-draft-part-2-21199a2de18a\">Read this article in english</a></p>\n</blockquote>","date":"2015-01-27T09:08:00.000Z","pubdate":"2015-01-27T09:08:00.000Z","pubDate":"2015-01-27T09:08:00.000Z","link":"http://blog.michaelwittig.info/spaltenorientierte-datenbank-konzept-teil-2/","guid":"a8d0928f-f311-4627-8de7-08599eec458b","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["spaltenorientiert","Daten","BigData","Datenbank","Reihe1"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Spaltenorientierte Datenbank: Konzept (Teil 2)"},"rss:description":{"@":{},"#":"<p>Im 2. Teil wollen wir uns detaillierter mit dem Konzept einer spaltenorientierte Datenbank beschäftigen. Als Beispiel entwickeln wir das Konzept für eine Time Series Database (TSDB). Im <a href=\"http://blog.michaelwittig.info/spaltenorientierte-datenbank-intro-teil-1/\">Teil 1</a> der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> haben wir uns den Unterschied zwischen zeilenorientiert und spaltenorientiert angeschaut. </p>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-draft-part-2-21199a2de18a\">Read this article in english</a></p>\n</blockquote>\n\n<h2 id=\"ondiskkonzept\">On-Disk Konzept</h2>\n\n<p>Die Datenbank ist spaltenorientiert. Das bedeutet, dass jede Spalte als Datei repräsentiert wird. Die erste Spalte ist immer der Zeitstempel. Gefolgt von mehreren dazugehörigen Werten.</p>\n\n<pre><code>Zeitstempel     Wert1     Wert2\n====================================\n10:01:00.000    324       34654\n11:05:00.000    342       23463\n12:17:00.000    446       47232\n13:03:00.000    234       60383\n14:31:00.000    213       73947\n15:14:00.000    342       46286\n</code></pre>\n\n<p>Wichtig ist, dass die Einträge aufsteigend nach Zeitstempel sortiert sind. Werden neue Daten hinzugefügt müssen diese also nur ans Ende der Dateien angehängt werden und schon sind sie sortiert. Eine Weitere Einschränkung ist, dass Spalten nur gleiche Typen beinhalten dürfen. Für den Anfang wird der Zeitstempel als 32-bit unsigned Integer abgespeichert und alle Werte als 32-bit signed Float.</p>\n\n<p>Die wichtigste Abfrage wird eine Selektion auf die Zeitstempel sein. Da diese sortiert vorliegen können wir eine <a href=\"http://de.wikipedia.org/wiki/Binäre_Suche\">Binäre Suche</a> nutzen um effizient den Anfang der Zeitreihe in der Datei zu finden. Da Werte alle die gleiche Länge (32-bit) haben, kann auch in den Dateien der Werte an die richtige Position gesprungen werden.</p>\n\n<p>Damit auch sehr große Zeitreihen unterstützt werden können bietet sich eine Partitionierung an. Entweder eine fixe Partitionierung z. B. ein Tag oder auch eine bestimmte Anzahl an Elementen.</p>\n\n<pre><code>/\n    2014-01-01/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n    2014-01-02/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n</code></pre>\n\n<p>Oder eine variable Partitionierung .z B. gibt der Ordnername den ersten Zeitstempel an.</p>\n\n<pre><code>  /\n    10:01:00.000/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n    14:31:00.000/\n        Zeitstempel.dat\n        Wert1.dat\n        Wert2.dat\n</code></pre>\n\n<h2 id=\"writekonzept\">Write Konzept</h2>\n\n<p>Da wir in mehrere Dateien gleichzeitig schreiben müssen (Zeitstempel.dat, Wert1.dat, Wert2.dat, ...) und diese  gleich viele Einträge haben müssen, muss zudem sichergestellt sein, dass während eines Schreibvorganges kein Lesezugriff erfolgen kann oder dieser immer defensiv erfolgt. So könnte zum Beispiel immer zuerst die Zeitstempel.dat gelesen werden. Geschrieben wird aber immer zuerst in die Wert2.dat. So kann es nie vorkommen, dass Zeitstempel existieren für die es noch keine Werte gibt. Diese Methode ist wesentlich effizienter, da <a href=\"http://de.wikipedia.org/wiki/Nicht-blockierende_Synchronisation\">keine künstliche Synchronisation</a> benötigt wird.</p>\n\n<h2 id=\"readkonzept\">Read Konzept</h2>\n\n<p>Um Daten möglichst schnell zu lesen müssen so wenig wie möglich Daten gelesen werden. Das gewählte On-Disk Konzept erfüllt diese Voraussetzung. Evtl. kann noch über eine Komprimierung nachgedacht werden, da das Entpacken bis zu einem gewissen Grad schneller ist als die Datenmenge von langsamen Festplatten zu lesen.</p>\n\n<p>Des weiteren muss noch darauf geachtet werden, dass niemals die gesamte Datenmenge in den Speicher passen muss. Sog. Streaming Algorithmen erfüllen diesen Zweck.</p>\n\n<p>Als Caching Mechanismus benutzen wir die vom Betriebsystem bereitgestellten Mechanismen für IO Caching. So werden Anfragen auf den gleichen Datenbestand deutlich schneller bearbeitet, da nicht von Festplatte gelesen werden muss.</p>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Durch die Verwendung von normalen Dateien und ein paar Tricks lässt sich eine gute Grundlage für eine Time Series Database legen. Durch die Ausnutzung der Möglichkeiten ist es möglich mit relativ wenig Aufwand ein großes Datenbanksystem zu betrieben.</p>\n\n<ul>\n<li>TimeSeries.Guru ist eine spaltenorientierte <a href=\"http://www.timeseries.guru/\">Time Series Database as a Service</a></li>\n<li>MonetDB ist eine <a href=\"https://www.monetdb.org/\">Open Source spaltenorientierte Datenbank</a></li>\n<li>Wikipedia Artikel zu <a href=\"http://de.wikipedia.org/wiki/Spaltenorientierte_Datenbank\">Spaltenorientierte Datenbank</a></li>\n</ul>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-draft-part-2-21199a2de18a\">Read this article in english</a></p>\n</blockquote>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/spaltenorientierte-datenbank-konzept-teil-2/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"a8d0928f-f311-4627-8de7-08599eec458b"},"rss:category":[{"@":{},"#":"spaltenorientiert"},{"@":{},"#":"Daten"},{"@":{},"#":"BigData"},{"@":{},"#":"Datenbank"},{"@":{},"#":"Reihe1"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Tue, 27 Jan 2015 09:08:00 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Streams in Java 8","description":"<p>Mit Java 8 stehen uns <a href=\"http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html\">Streams</a> in Kombination mit <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html\">Lambdas</a> zur Verfügung um funktional Operationen auf ein Datenstrom von Elementen anzuwenden.</p>\n\n<h2 id=\"stream\">Stream</h2>\n\n<ul>\n<li>Ein Stream ist kein Datenspeicher, sondern eine Sicht auf einen Datenspeicher</li>\n<li>Operationen auf eine Stream erzeugen ein neues Ergebnis ohne die ursprünglichen Daten zu verändern</li>\n<li>Streams sind <a href=\"http://de.wikipedia.org/wiki/Lazy_Evaluation\">lazy</a></li>\n<li>Streams können von unendlicher Länge sein</li>\n<li>Die Elememte in einem Stream sind nur einmal sichtbar (vgl. <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html\">Iterator</a>)</li>\n</ul>\n\n<h2 id=\"streampipeline\">Stream Pipeline</h2>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/12/Stream-pipeline.png\" alt=\"Stream Pipeline\"></p>\n\n<p>Eine Stream Pipeline besteht aus einer <strong>Quelle</strong> (wie zum Beispiel einem Array, einer Collection, einem Generator, einem I/O Channel, ...). <br>\nDarauf folgen optional <strong>Zwischen Operationen</strong> welche einen Stream in einen anderen Stream transformieren (wie z. B. filter()). <br>\nAm Ende steht eine <strong>Terminierende Operation</strong> die ein Ergebnis oder ein Seiten-Effekt erzeugt (wie z. B. count() oder forEach()).</p>\n\n<h2 id=\"beispiele\">Beispiele</h2>\n\n<h3 id=\"filternundsummieren\">Filtern und summieren</h3>\n\n<p>Zum Beginn wollen wir uns ein einfaches Beispiel anschauen, bei dem die Summe der geraden natürlichen Zahlen &lt; 10000 gebildet werden soll.</p>\n\n<h4 id=\"ohnestreams\">ohne Streams</h4>\n\n<pre><code>// erzeuge Zahlen\nfinal List&lt;Integer&gt; zahlen = new ArrayList&lt;&gt;();\nfor (int i = 0; i &lt; 10000; i++) {\n    zahlen.add(i);\n}\n\n// filter gerade Zahlen\nfinal List&lt;Integer&gt; geradeZahlen = new ArrayList&lt;&gt;();\nfor (final Integer zahl : zahlen) {\n    if (zahl % 2 == 0) {\n        geradeZahlen.add(zahl);\n    }\n}\n\n// summiere\nint summeGeradeZahlen = 0;\nfor (final Integer geradeZahl : geradeZahlen) {\n    summeGeradeZahlen += geradeZahl;\n}\nSystem.out.println(\"summeGeradeZahlen \" + summeGeradeZahlen);\n</code></pre>\n\n<h4 id=\"mitstreams\">mit Streams</h4>\n\n<pre><code>int summeGeradeZahlen = IntStream.range(0, 10000) // Quelle (Generator): erzeuge Zahlen\n    .filter(zahl -&gt; zahl % 2 == 0)                // Zwischen Operation: filter gerade Zahlen\n    .sum();                                       // Terminierende Operation: summiere\nSystem.out.println(\"summeGeradeZahlen \" + summeGeradeZahlen);\n</code></pre>\n\n<h3 id=\"laziness\">Laziness</h3>\n\n<p>Der Vorteil an Streams ist, dass die zwischen Ergebnisse erst dann errechnet werden, wenn sie wirklich benötigt werden. Der folgende Code-Abschnitt hat keine terminierende Operation und wird daher nie \"ausgerechnet\".</p>\n\n<pre><code>new Random().ints()                 // Quelle (Generator)\n    .limit(10000)                   // Zwischen Operation\n    .filter(zahl -&gt; zahl % 2 == 0); // Zwischen Operation\n</code></pre>\n\n<p>Das folgende Beispiel schließt mit <code>average()</code> ab was eine terminierende Operation ist. Beim aufruf von <code>average()</code> werden nun 10.000 Zufallszahlen erzeugt und nur die geraden zur Berechnung des Durchschnitts berücksichtigt.</p>\n\n<pre><code>new Random().ints()                 // Quelle (Generator)\n    .limit(10000)                   // Zwischen Operation\n    .filter(zahl -&gt; zahl % 2 == 0); // Zwischen Operation\n    .average();                     // Terminierende Operation\n</code></pre>\n\n<p>Durch die Lazy Evaluation werden auch unendliche Reihen möglich. Folgendes Beispiel erzeugt eine unendliche Reihe der 10er Zahlen (0, 10, 20, 30, 40, 50, ...). Es werden aber in Wirklichkeit nur 10 Zahlen erzeugt, da die ersten 10.000.000 übersprungen werden und auf 10 Zahlen limitiert wird.</p>\n\n<pre><code>Stream.iterate(0, n -&gt; n + 10)                // Quelle (Generator)\n  .skip(10000000)                             // Zwischen Operation\n  .limit(10)                                  // Zwischen Operation\n  .sorted(Comparator.&lt;Integer&gt;reverseOrder()) // Zwischen Operation\n  .forEach(System.out::println);              // Terminierende Operation\n</code></pre>\n\n<p>Wichtig ist, dass keine terminierende Operation auf den unendlichen Stream angwendet werden. Der folgende Code ist eine Endlosschleife!</p>\n\n<pre><code>Stream.iterate(0, n -&gt; n + 10).average();\n</code></pre>\n\n<h3 id=\"querysprache\">Query Sprache</h3>\n\n<p>Folgende Entität sei vorhanden:</p>\n\n<pre><code>public static final class Person implements Comparable&lt;Person&gt; {\n  private final String name;\n  private final String ort;\n  private final Geschlecht geschlecht;\n\n  public Person(final String name, final String ort, final Geschlecht geschlecht) {\n      this.name = name;\n      this.ort = ort;\n      this.geschlecht = geschlecht;\n  }\n\n   public String getOrt() { return this.ort; }\n   public int compareTo(final Person o) { return this.name.compareTo(o.name); }\n}\n</code></pre>\n\n<p>Unser Datenbestand umfasst nun etliche Personen die in einer <code>List&lt;Person&gt; personen = ...</code> gespeichert sind. Wir wollen nun alle männlichen Personen finden und Sie natürlich ordnen. Das Ergebnis soll auf Seiten mit je 50 Personen ausgegeben werden,</p>\n\n<pre><code>int page = 0;\npersonen.stream()\n    .filter(person -&gt; person.geschlecht == Geschlecht.männlich)\n    .sorted(Comparator.naturalOrder())\n    .skip(page * 50)\n    .limit(50)\n    .forEach(System.out::println);\n</code></pre>\n\n<p>Interessant wäre auch zu wissen, wieviele Personen an welchem Ort wohnen.</p>\n\n<pre><code>final Map&lt;String, Long&gt; orte = PERSONEN.stream()\n    .collect(Collectors.groupingBy(\n            Person::getOrt, Collectors.counting()\n    ));\nSystem.out.println(\"orte \" + orte);\n</code></pre>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Mit Streams und Lambdas lassen sich Schleifen und Verzweigungen sehr kompakt darstellen.</p>","summary":"<p>Mit Java 8 stehen uns <a href=\"http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html\">Streams</a> in Kombination mit <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html\">Lambdas</a> zur Verfügung um funktional Operationen auf ein Datenstrom von Elementen anzuwenden.</p>\n\n<h2 id=\"stream\">Stream</h2>\n\n<ul>\n<li>Ein Stream ist kein Datenspeicher, sondern eine Sicht auf einen Datenspeicher</li>\n<li>Operationen auf eine Stream erzeugen ein neues Ergebnis ohne die ursprünglichen Daten zu verändern</li>\n<li>Streams sind <a href=\"http://de.wikipedia.org/wiki/Lazy_Evaluation\">lazy</a></li>\n<li>Streams können von unendlicher Länge sein</li>\n<li>Die Elememte in einem Stream sind nur einmal sichtbar (vgl. <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html\">Iterator</a>)</li>\n</ul>\n\n<h2 id=\"streampipeline\">Stream Pipeline</h2>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/12/Stream-pipeline.png\" alt=\"Stream Pipeline\"></p>\n\n<p>Eine Stream Pipeline besteht aus einer <strong>Quelle</strong> (wie zum Beispiel einem Array, einer Collection, einem Generator, einem I/O Channel, ...). <br>\nDarauf folgen optional <strong>Zwischen Operationen</strong> welche einen Stream in einen anderen Stream transformieren (wie z. B. filter()). <br>\nAm Ende steht eine <strong>Terminierende Operation</strong> die ein Ergebnis oder ein Seiten-Effekt erzeugt (wie z. B. count() oder forEach()).</p>\n\n<h2 id=\"beispiele\">Beispiele</h2>\n\n<h3 id=\"filternundsummieren\">Filtern und summieren</h3>\n\n<p>Zum Beginn wollen wir uns ein einfaches Beispiel anschauen, bei dem die Summe der geraden natürlichen Zahlen &lt; 10000 gebildet werden soll.</p>\n\n<h4 id=\"ohnestreams\">ohne Streams</h4>\n\n<pre><code>// erzeuge Zahlen\nfinal List&lt;Integer&gt; zahlen = new ArrayList&lt;&gt;();\nfor (int i = 0; i &lt; 10000; i++) {\n    zahlen.add(i);\n}\n\n// filter gerade Zahlen\nfinal List&lt;Integer&gt; geradeZahlen = new ArrayList&lt;&gt;();\nfor (final Integer zahl : zahlen) {\n    if (zahl % 2 == 0) {\n        geradeZahlen.add(zahl);\n    }\n}\n\n// summiere\nint summeGeradeZahlen = 0;\nfor (final Integer geradeZahl : geradeZahlen) {\n    summeGeradeZahlen += geradeZahl;\n}\nSystem.out.println(\"summeGeradeZahlen \" + summeGeradeZahlen);\n</code></pre>\n\n<h4 id=\"mitstreams\">mit Streams</h4>\n\n<pre><code>int summeGeradeZahlen = IntStream.range(0, 10000) // Quelle (Generator): erzeuge Zahlen\n    .filter(zahl -&gt; zahl % 2 == 0)                // Zwischen Operation: filter gerade Zahlen\n    .sum();                                       // Terminierende Operation: summiere\nSystem.out.println(\"summeGeradeZahlen \" + summeGeradeZahlen);\n</code></pre>\n\n<h3 id=\"laziness\">Laziness</h3>\n\n<p>Der Vorteil an Streams ist, dass die zwischen Ergebnisse erst dann errechnet werden, wenn sie wirklich benötigt werden. Der folgende Code-Abschnitt hat keine terminierende Operation und wird daher nie \"ausgerechnet\".</p>\n\n<pre><code>new Random().ints()                 // Quelle (Generator)\n    .limit(10000)                   // Zwischen Operation\n    .filter(zahl -&gt; zahl % 2 == 0); // Zwischen Operation\n</code></pre>\n\n<p>Das folgende Beispiel schließt mit <code>average()</code> ab was eine terminierende Operation ist. Beim aufruf von <code>average()</code> werden nun 10.000 Zufallszahlen erzeugt und nur die geraden zur Berechnung des Durchschnitts berücksichtigt.</p>\n\n<pre><code>new Random().ints()                 // Quelle (Generator)\n    .limit(10000)                   // Zwischen Operation\n    .filter(zahl -&gt; zahl % 2 == 0); // Zwischen Operation\n    .average();                     // Terminierende Operation\n</code></pre>\n\n<p>Durch die Lazy Evaluation werden auch unendliche Reihen möglich. Folgendes Beispiel erzeugt eine unendliche Reihe der 10er Zahlen (0, 10, 20, 30, 40, 50, ...). Es werden aber in Wirklichkeit nur 10 Zahlen erzeugt, da die ersten 10.000.000 übersprungen werden und auf 10 Zahlen limitiert wird.</p>\n\n<pre><code>Stream.iterate(0, n -&gt; n + 10)                // Quelle (Generator)\n  .skip(10000000)                             // Zwischen Operation\n  .limit(10)                                  // Zwischen Operation\n  .sorted(Comparator.&lt;Integer&gt;reverseOrder()) // Zwischen Operation\n  .forEach(System.out::println);              // Terminierende Operation\n</code></pre>\n\n<p>Wichtig ist, dass keine terminierende Operation auf den unendlichen Stream angwendet werden. Der folgende Code ist eine Endlosschleife!</p>\n\n<pre><code>Stream.iterate(0, n -&gt; n + 10).average();\n</code></pre>\n\n<h3 id=\"querysprache\">Query Sprache</h3>\n\n<p>Folgende Entität sei vorhanden:</p>\n\n<pre><code>public static final class Person implements Comparable&lt;Person&gt; {\n  private final String name;\n  private final String ort;\n  private final Geschlecht geschlecht;\n\n  public Person(final String name, final String ort, final Geschlecht geschlecht) {\n      this.name = name;\n      this.ort = ort;\n      this.geschlecht = geschlecht;\n  }\n\n   public String getOrt() { return this.ort; }\n   public int compareTo(final Person o) { return this.name.compareTo(o.name); }\n}\n</code></pre>\n\n<p>Unser Datenbestand umfasst nun etliche Personen die in einer <code>List&lt;Person&gt; personen = ...</code> gespeichert sind. Wir wollen nun alle männlichen Personen finden und Sie natürlich ordnen. Das Ergebnis soll auf Seiten mit je 50 Personen ausgegeben werden,</p>\n\n<pre><code>int page = 0;\npersonen.stream()\n    .filter(person -&gt; person.geschlecht == Geschlecht.männlich)\n    .sorted(Comparator.naturalOrder())\n    .skip(page * 50)\n    .limit(50)\n    .forEach(System.out::println);\n</code></pre>\n\n<p>Interessant wäre auch zu wissen, wieviele Personen an welchem Ort wohnen.</p>\n\n<pre><code>final Map&lt;String, Long&gt; orte = PERSONEN.stream()\n    .collect(Collectors.groupingBy(\n            Person::getOrt, Collectors.counting()\n    ));\nSystem.out.println(\"orte \" + orte);\n</code></pre>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Mit Streams und Lambdas lassen sich Schleifen und Verzweigungen sehr kompakt darstellen.</p>","date":"2014-12-19T12:00:06.000Z","pubdate":"2014-12-19T12:00:06.000Z","pubDate":"2014-12-19T12:00:06.000Z","link":"http://blog.michaelwittig.info/streams-in-java-8/","guid":"ede6bdb7-833e-4b2d-94c6-1b8dd50fac78","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["Java","Stream","Lambda"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Streams in Java 8"},"rss:description":{"@":{},"#":"<p>Mit Java 8 stehen uns <a href=\"http://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html\">Streams</a> in Kombination mit <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html\">Lambdas</a> zur Verfügung um funktional Operationen auf ein Datenstrom von Elementen anzuwenden.</p>\n\n<h2 id=\"stream\">Stream</h2>\n\n<ul>\n<li>Ein Stream ist kein Datenspeicher, sondern eine Sicht auf einen Datenspeicher</li>\n<li>Operationen auf eine Stream erzeugen ein neues Ergebnis ohne die ursprünglichen Daten zu verändern</li>\n<li>Streams sind <a href=\"http://de.wikipedia.org/wiki/Lazy_Evaluation\">lazy</a></li>\n<li>Streams können von unendlicher Länge sein</li>\n<li>Die Elememte in einem Stream sind nur einmal sichtbar (vgl. <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html\">Iterator</a>)</li>\n</ul>\n\n<h2 id=\"streampipeline\">Stream Pipeline</h2>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/12/Stream-pipeline.png\" alt=\"Stream Pipeline\"></p>\n\n<p>Eine Stream Pipeline besteht aus einer <strong>Quelle</strong> (wie zum Beispiel einem Array, einer Collection, einem Generator, einem I/O Channel, ...). <br>\nDarauf folgen optional <strong>Zwischen Operationen</strong> welche einen Stream in einen anderen Stream transformieren (wie z. B. filter()). <br>\nAm Ende steht eine <strong>Terminierende Operation</strong> die ein Ergebnis oder ein Seiten-Effekt erzeugt (wie z. B. count() oder forEach()).</p>\n\n<h2 id=\"beispiele\">Beispiele</h2>\n\n<h3 id=\"filternundsummieren\">Filtern und summieren</h3>\n\n<p>Zum Beginn wollen wir uns ein einfaches Beispiel anschauen, bei dem die Summe der geraden natürlichen Zahlen &lt; 10000 gebildet werden soll.</p>\n\n<h4 id=\"ohnestreams\">ohne Streams</h4>\n\n<pre><code>// erzeuge Zahlen\nfinal List&lt;Integer&gt; zahlen = new ArrayList&lt;&gt;();\nfor (int i = 0; i &lt; 10000; i++) {\n    zahlen.add(i);\n}\n\n// filter gerade Zahlen\nfinal List&lt;Integer&gt; geradeZahlen = new ArrayList&lt;&gt;();\nfor (final Integer zahl : zahlen) {\n    if (zahl % 2 == 0) {\n        geradeZahlen.add(zahl);\n    }\n}\n\n// summiere\nint summeGeradeZahlen = 0;\nfor (final Integer geradeZahl : geradeZahlen) {\n    summeGeradeZahlen += geradeZahl;\n}\nSystem.out.println(\"summeGeradeZahlen \" + summeGeradeZahlen);\n</code></pre>\n\n<h4 id=\"mitstreams\">mit Streams</h4>\n\n<pre><code>int summeGeradeZahlen = IntStream.range(0, 10000) // Quelle (Generator): erzeuge Zahlen\n    .filter(zahl -&gt; zahl % 2 == 0)                // Zwischen Operation: filter gerade Zahlen\n    .sum();                                       // Terminierende Operation: summiere\nSystem.out.println(\"summeGeradeZahlen \" + summeGeradeZahlen);\n</code></pre>\n\n<h3 id=\"laziness\">Laziness</h3>\n\n<p>Der Vorteil an Streams ist, dass die zwischen Ergebnisse erst dann errechnet werden, wenn sie wirklich benötigt werden. Der folgende Code-Abschnitt hat keine terminierende Operation und wird daher nie \"ausgerechnet\".</p>\n\n<pre><code>new Random().ints()                 // Quelle (Generator)\n    .limit(10000)                   // Zwischen Operation\n    .filter(zahl -&gt; zahl % 2 == 0); // Zwischen Operation\n</code></pre>\n\n<p>Das folgende Beispiel schließt mit <code>average()</code> ab was eine terminierende Operation ist. Beim aufruf von <code>average()</code> werden nun 10.000 Zufallszahlen erzeugt und nur die geraden zur Berechnung des Durchschnitts berücksichtigt.</p>\n\n<pre><code>new Random().ints()                 // Quelle (Generator)\n    .limit(10000)                   // Zwischen Operation\n    .filter(zahl -&gt; zahl % 2 == 0); // Zwischen Operation\n    .average();                     // Terminierende Operation\n</code></pre>\n\n<p>Durch die Lazy Evaluation werden auch unendliche Reihen möglich. Folgendes Beispiel erzeugt eine unendliche Reihe der 10er Zahlen (0, 10, 20, 30, 40, 50, ...). Es werden aber in Wirklichkeit nur 10 Zahlen erzeugt, da die ersten 10.000.000 übersprungen werden und auf 10 Zahlen limitiert wird.</p>\n\n<pre><code>Stream.iterate(0, n -&gt; n + 10)                // Quelle (Generator)\n  .skip(10000000)                             // Zwischen Operation\n  .limit(10)                                  // Zwischen Operation\n  .sorted(Comparator.&lt;Integer&gt;reverseOrder()) // Zwischen Operation\n  .forEach(System.out::println);              // Terminierende Operation\n</code></pre>\n\n<p>Wichtig ist, dass keine terminierende Operation auf den unendlichen Stream angwendet werden. Der folgende Code ist eine Endlosschleife!</p>\n\n<pre><code>Stream.iterate(0, n -&gt; n + 10).average();\n</code></pre>\n\n<h3 id=\"querysprache\">Query Sprache</h3>\n\n<p>Folgende Entität sei vorhanden:</p>\n\n<pre><code>public static final class Person implements Comparable&lt;Person&gt; {\n  private final String name;\n  private final String ort;\n  private final Geschlecht geschlecht;\n\n  public Person(final String name, final String ort, final Geschlecht geschlecht) {\n      this.name = name;\n      this.ort = ort;\n      this.geschlecht = geschlecht;\n  }\n\n   public String getOrt() { return this.ort; }\n   public int compareTo(final Person o) { return this.name.compareTo(o.name); }\n}\n</code></pre>\n\n<p>Unser Datenbestand umfasst nun etliche Personen die in einer <code>List&lt;Person&gt; personen = ...</code> gespeichert sind. Wir wollen nun alle männlichen Personen finden und Sie natürlich ordnen. Das Ergebnis soll auf Seiten mit je 50 Personen ausgegeben werden,</p>\n\n<pre><code>int page = 0;\npersonen.stream()\n    .filter(person -&gt; person.geschlecht == Geschlecht.männlich)\n    .sorted(Comparator.naturalOrder())\n    .skip(page * 50)\n    .limit(50)\n    .forEach(System.out::println);\n</code></pre>\n\n<p>Interessant wäre auch zu wissen, wieviele Personen an welchem Ort wohnen.</p>\n\n<pre><code>final Map&lt;String, Long&gt; orte = PERSONEN.stream()\n    .collect(Collectors.groupingBy(\n            Person::getOrt, Collectors.counting()\n    ));\nSystem.out.println(\"orte \" + orte);\n</code></pre>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Mit Streams und Lambdas lassen sich Schleifen und Verzweigungen sehr kompakt darstellen.</p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/streams-in-java-8/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"ede6bdb7-833e-4b2d-94c6-1b8dd50fac78"},"rss:category":[{"@":{},"#":"Java"},{"@":{},"#":"Stream"},{"@":{},"#":"Lambda"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Fri, 19 Dec 2014 12:00:06 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Partitionierung und Segmentierung in kdb+","description":"<p>Big Data ist in aller Munde. Die spaltenorientierte Datenbank kdb+ geht dieses Problem mit Partitionierung und Segmentierung an. Bei Zeitreihen eignet sich eine Aufteilung in Zeitfenster wie Monat, Woche oder Tag. Die einzelnen Partitionen können dann für die meisten Probleme parallel mit <a href=\"http://de.wikipedia.org/wiki/MapReduce\">Map/Reduce</a> verarbeitet werden. Als kleine Auffrischung wie kdb+ Daten speichert dient der Artikel <a href=\"http://blog.michaelwittig.info/die-spaltenorientierte-datenbank-kdb/\">Die spaltenorientierte Datenbank kdb+</a>.</p>\n\n<h2 id=\"partitionierung\">Partitionierung</h2>\n\n<p>Im folgenden Beispiel wurde nach Jahr partitioniert:</p>\n\n<pre><code>./db/\n    sym\n    ./2014/\n        ./prices/\n            .d\n            price\n            sym\n            time\n    ./2013/\n        ./prices/\n            .d\n            price\n            sym\n            time\n</code></pre>\n\n<p>Zwei Dinge sind interessant:</p>\n\n<h3 id=\"abfragen\">Abfragen</h3>\n\n<pre><code>select price from prices where year=2014,sym=`TEST\n</code></pre>\n\n<p>Bei einer normalen splayed Table (ohne Sortierung oder sonstige <a href=\"http://code.kx.com/wiki/JB:QforMortals/tables#Attributes\">Attributes</a>) müssen auch die Daten von 2013 gelesen werden um festzustellen, dass diese nicht von Interesse sind. Die Query gegen eine partitioned Table wird aber nur auf die Daten von 2014 angewendet. <strong>Wichtig ist, dass die Einschränkung auf die Partition als erstes im where Teil steht!</strong></p>\n\n<h3 id=\"mapreduce\">Map/Reduce</h3>\n\n<pre><code>select min price from prices where sym=`TEST\n</code></pre>\n\n<p>Um das Minimum der Preis Spalte zu finden wird das Minimum für 2014 und 2013 berechnet. In einem weiteren Schritt dann das Minimum der zwei Vorgänger Werte. Der Clou ist, dass die Berechnung für das Minimum von 2013 und 2014 unabhängig sind und somit parallel erfolgen können (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables#1.3.4_Query_Execution_on_Partitioned_Tables_.28Advanced.29\">mehr dazu</a>).</p>\n\n<h2 id=\"segmentierung\">Segmentierung</h2>\n\n<p>Segmentierung erweitert die Partitionierung um eine weitere Ebene. </p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/11/segmentation.gif\" alt=\"Segmentation\"></p>\n\n<p>Eine Möglichkeit zur Segemtnierung in zwei Segmente wäre zum Beispiel ob das Datum gerade oder ungerade endet. Wenn Segmente physisch auf verschiedenen Festplatten (IO Channels) liegen, wird kdb+ parallel von mehreren Festplatten lesen und via Map/Reduce verarbeiten (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4.4_Query_Execution_on_Segmented_Tables_.28Advanced.29\">mehr dazu</a>).</p>\n\n<p>Ein Segment kann nicht wie eine Partition über die Query angesprochen werden. kdb+ kümmert sich selber darum die richtigen Segmente zu finden die für eine Query benötigt werden (<a href=\"http://code.kx.com/wiki/DotQ/DotQDotD\">mehr dazu</a>).</p>\n\n<p>Eine Tabelle kann auch in der gleichen Partition über mehrer Segmente verteilt werden. So könnte bei einer Partitionierung nach Datum zum Beispiel alle Werte bis inklusive 12:00 in Segment A liegen, alle Daten ab exklusive 12:00 in Segment B. Es muss nur darauf geachtet werden, dass niemals Daten doppelt in Segemtnen liegen (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4.3_Multiple_Segmented_Tables\">mehr dazu</a>).</p>\n\n<h3 id=\"optimalesegmentierung\">optimale Segmentierung?</h3>\n\n<p>Eine optimale Segmentierung ist erreicht, wenn für die Anfrage möglichst alle IO Channels arbeiten. Schlecht wäre zum Beispiel eine Segmentierung nach Monat wenn die meisten Queries nur an den Daten der letzen 7 Tage interessiert sind, da somit viele Lesezugriffe auf dem gleichen Segment liegen würden (zur Vereinfachung ohne IO Cache). Auch der Wochentag (Montag - Sonntag) kann ungeignet sein, da Montag - Freitag evtl. mehr Daten erzeugt werden als Samstag - Sonntag.</p>\n\n<p>Eine allgemeine optimale Segmentierung ist in den meisten Fällen nicht möglich. Anbieten tut sich zum Beispiel der Tag des Jahres Modulo 3 oder Modulo 5 für eine allgemeine Regel.</p>\n\n<p>Für bestimmte Workloads hat man immer die Option Daten zu kopieren und ensprechend den Anfragen in Segmenten abzulegen.</p>\n\n<h2 id=\"zusammenfassung\">Zusammenfassung</h2>\n\n<ul>\n<li>Segmente werden parallel gelesen (multiple I/O channels)</li>\n<li>Es werdne nur benötigte Partitionen gelesen</li>\n<li>Map/Reduce für Partitionen</li>\n<li>Es werden nur benötigte Spalten gelesen</li>\n<li>Q muss mit der Option <code>q -s N</code> für <code>N</code> Threads gestartet werden um parallelle Verarbeitung zu aktivieren</li>\n</ul>","summary":"<p>Big Data ist in aller Munde. Die spaltenorientierte Datenbank kdb+ geht dieses Problem mit Partitionierung und Segmentierung an. Bei Zeitreihen eignet sich eine Aufteilung in Zeitfenster wie Monat, Woche oder Tag. Die einzelnen Partitionen können dann für die meisten Probleme parallel mit <a href=\"http://de.wikipedia.org/wiki/MapReduce\">Map/Reduce</a> verarbeitet werden. Als kleine Auffrischung wie kdb+ Daten speichert dient der Artikel <a href=\"http://blog.michaelwittig.info/die-spaltenorientierte-datenbank-kdb/\">Die spaltenorientierte Datenbank kdb+</a>.</p>\n\n<h2 id=\"partitionierung\">Partitionierung</h2>\n\n<p>Im folgenden Beispiel wurde nach Jahr partitioniert:</p>\n\n<pre><code>./db/\n    sym\n    ./2014/\n        ./prices/\n            .d\n            price\n            sym\n            time\n    ./2013/\n        ./prices/\n            .d\n            price\n            sym\n            time\n</code></pre>\n\n<p>Zwei Dinge sind interessant:</p>\n\n<h3 id=\"abfragen\">Abfragen</h3>\n\n<pre><code>select price from prices where year=2014,sym=`TEST\n</code></pre>\n\n<p>Bei einer normalen splayed Table (ohne Sortierung oder sonstige <a href=\"http://code.kx.com/wiki/JB:QforMortals/tables#Attributes\">Attributes</a>) müssen auch die Daten von 2013 gelesen werden um festzustellen, dass diese nicht von Interesse sind. Die Query gegen eine partitioned Table wird aber nur auf die Daten von 2014 angewendet. <strong>Wichtig ist, dass die Einschränkung auf die Partition als erstes im where Teil steht!</strong></p>\n\n<h3 id=\"mapreduce\">Map/Reduce</h3>\n\n<pre><code>select min price from prices where sym=`TEST\n</code></pre>\n\n<p>Um das Minimum der Preis Spalte zu finden wird das Minimum für 2014 und 2013 berechnet. In einem weiteren Schritt dann das Minimum der zwei Vorgänger Werte. Der Clou ist, dass die Berechnung für das Minimum von 2013 und 2014 unabhängig sind und somit parallel erfolgen können (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables#1.3.4_Query_Execution_on_Partitioned_Tables_.28Advanced.29\">mehr dazu</a>).</p>\n\n<h2 id=\"segmentierung\">Segmentierung</h2>\n\n<p>Segmentierung erweitert die Partitionierung um eine weitere Ebene. </p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/11/segmentation.gif\" alt=\"Segmentation\"></p>\n\n<p>Eine Möglichkeit zur Segemtnierung in zwei Segmente wäre zum Beispiel ob das Datum gerade oder ungerade endet. Wenn Segmente physisch auf verschiedenen Festplatten (IO Channels) liegen, wird kdb+ parallel von mehreren Festplatten lesen und via Map/Reduce verarbeiten (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4.4_Query_Execution_on_Segmented_Tables_.28Advanced.29\">mehr dazu</a>).</p>\n\n<p>Ein Segment kann nicht wie eine Partition über die Query angesprochen werden. kdb+ kümmert sich selber darum die richtigen Segmente zu finden die für eine Query benötigt werden (<a href=\"http://code.kx.com/wiki/DotQ/DotQDotD\">mehr dazu</a>).</p>\n\n<p>Eine Tabelle kann auch in der gleichen Partition über mehrer Segmente verteilt werden. So könnte bei einer Partitionierung nach Datum zum Beispiel alle Werte bis inklusive 12:00 in Segment A liegen, alle Daten ab exklusive 12:00 in Segment B. Es muss nur darauf geachtet werden, dass niemals Daten doppelt in Segemtnen liegen (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4.3_Multiple_Segmented_Tables\">mehr dazu</a>).</p>\n\n<h3 id=\"optimalesegmentierung\">optimale Segmentierung?</h3>\n\n<p>Eine optimale Segmentierung ist erreicht, wenn für die Anfrage möglichst alle IO Channels arbeiten. Schlecht wäre zum Beispiel eine Segmentierung nach Monat wenn die meisten Queries nur an den Daten der letzen 7 Tage interessiert sind, da somit viele Lesezugriffe auf dem gleichen Segment liegen würden (zur Vereinfachung ohne IO Cache). Auch der Wochentag (Montag - Sonntag) kann ungeignet sein, da Montag - Freitag evtl. mehr Daten erzeugt werden als Samstag - Sonntag.</p>\n\n<p>Eine allgemeine optimale Segmentierung ist in den meisten Fällen nicht möglich. Anbieten tut sich zum Beispiel der Tag des Jahres Modulo 3 oder Modulo 5 für eine allgemeine Regel.</p>\n\n<p>Für bestimmte Workloads hat man immer die Option Daten zu kopieren und ensprechend den Anfragen in Segmenten abzulegen.</p>\n\n<h2 id=\"zusammenfassung\">Zusammenfassung</h2>\n\n<ul>\n<li>Segmente werden parallel gelesen (multiple I/O channels)</li>\n<li>Es werdne nur benötigte Partitionen gelesen</li>\n<li>Map/Reduce für Partitionen</li>\n<li>Es werden nur benötigte Spalten gelesen</li>\n<li>Q muss mit der Option <code>q -s N</code> für <code>N</code> Threads gestartet werden um parallelle Verarbeitung zu aktivieren</li>\n</ul>","date":"2014-11-14T14:00:13.000Z","pubdate":"2014-11-14T14:00:13.000Z","pubDate":"2014-11-14T14:00:13.000Z","link":"http://blog.michaelwittig.info/partitionierung-und-segmentierung-in-kdb/","guid":"4e026cba-c113-4ab7-adf9-8127fe2c0e2f","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["spaltenorientiert","Daten","kdb+","partitioned","segmented","BigData"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Partitionierung und Segmentierung in kdb+"},"rss:description":{"@":{},"#":"<p>Big Data ist in aller Munde. Die spaltenorientierte Datenbank kdb+ geht dieses Problem mit Partitionierung und Segmentierung an. Bei Zeitreihen eignet sich eine Aufteilung in Zeitfenster wie Monat, Woche oder Tag. Die einzelnen Partitionen können dann für die meisten Probleme parallel mit <a href=\"http://de.wikipedia.org/wiki/MapReduce\">Map/Reduce</a> verarbeitet werden. Als kleine Auffrischung wie kdb+ Daten speichert dient der Artikel <a href=\"http://blog.michaelwittig.info/die-spaltenorientierte-datenbank-kdb/\">Die spaltenorientierte Datenbank kdb+</a>.</p>\n\n<h2 id=\"partitionierung\">Partitionierung</h2>\n\n<p>Im folgenden Beispiel wurde nach Jahr partitioniert:</p>\n\n<pre><code>./db/\n    sym\n    ./2014/\n        ./prices/\n            .d\n            price\n            sym\n            time\n    ./2013/\n        ./prices/\n            .d\n            price\n            sym\n            time\n</code></pre>\n\n<p>Zwei Dinge sind interessant:</p>\n\n<h3 id=\"abfragen\">Abfragen</h3>\n\n<pre><code>select price from prices where year=2014,sym=`TEST\n</code></pre>\n\n<p>Bei einer normalen splayed Table (ohne Sortierung oder sonstige <a href=\"http://code.kx.com/wiki/JB:QforMortals/tables#Attributes\">Attributes</a>) müssen auch die Daten von 2013 gelesen werden um festzustellen, dass diese nicht von Interesse sind. Die Query gegen eine partitioned Table wird aber nur auf die Daten von 2014 angewendet. <strong>Wichtig ist, dass die Einschränkung auf die Partition als erstes im where Teil steht!</strong></p>\n\n<h3 id=\"mapreduce\">Map/Reduce</h3>\n\n<pre><code>select min price from prices where sym=`TEST\n</code></pre>\n\n<p>Um das Minimum der Preis Spalte zu finden wird das Minimum für 2014 und 2013 berechnet. In einem weiteren Schritt dann das Minimum der zwei Vorgänger Werte. Der Clou ist, dass die Berechnung für das Minimum von 2013 und 2014 unabhängig sind und somit parallel erfolgen können (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables#1.3.4_Query_Execution_on_Partitioned_Tables_.28Advanced.29\">mehr dazu</a>).</p>\n\n<h2 id=\"segmentierung\">Segmentierung</h2>\n\n<p>Segmentierung erweitert die Partitionierung um eine weitere Ebene. </p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/11/segmentation.gif\" alt=\"Segmentation\"></p>\n\n<p>Eine Möglichkeit zur Segemtnierung in zwei Segmente wäre zum Beispiel ob das Datum gerade oder ungerade endet. Wenn Segmente physisch auf verschiedenen Festplatten (IO Channels) liegen, wird kdb+ parallel von mehreren Festplatten lesen und via Map/Reduce verarbeiten (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4.4_Query_Execution_on_Segmented_Tables_.28Advanced.29\">mehr dazu</a>).</p>\n\n<p>Ein Segment kann nicht wie eine Partition über die Query angesprochen werden. kdb+ kümmert sich selber darum die richtigen Segmente zu finden die für eine Query benötigt werden (<a href=\"http://code.kx.com/wiki/DotQ/DotQDotD\">mehr dazu</a>).</p>\n\n<p>Eine Tabelle kann auch in der gleichen Partition über mehrer Segmente verteilt werden. So könnte bei einer Partitionierung nach Datum zum Beispiel alle Werte bis inklusive 12:00 in Segment A liegen, alle Daten ab exklusive 12:00 in Segment B. Es muss nur darauf geachtet werden, dass niemals Daten doppelt in Segemtnen liegen (<a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4.3_Multiple_Segmented_Tables\">mehr dazu</a>).</p>\n\n<h3 id=\"optimalesegmentierung\">optimale Segmentierung?</h3>\n\n<p>Eine optimale Segmentierung ist erreicht, wenn für die Anfrage möglichst alle IO Channels arbeiten. Schlecht wäre zum Beispiel eine Segmentierung nach Monat wenn die meisten Queries nur an den Daten der letzen 7 Tage interessiert sind, da somit viele Lesezugriffe auf dem gleichen Segment liegen würden (zur Vereinfachung ohne IO Cache). Auch der Wochentag (Montag - Sonntag) kann ungeignet sein, da Montag - Freitag evtl. mehr Daten erzeugt werden als Samstag - Sonntag.</p>\n\n<p>Eine allgemeine optimale Segmentierung ist in den meisten Fällen nicht möglich. Anbieten tut sich zum Beispiel der Tag des Jahres Modulo 3 oder Modulo 5 für eine allgemeine Regel.</p>\n\n<p>Für bestimmte Workloads hat man immer die Option Daten zu kopieren und ensprechend den Anfragen in Segmenten abzulegen.</p>\n\n<h2 id=\"zusammenfassung\">Zusammenfassung</h2>\n\n<ul>\n<li>Segmente werden parallel gelesen (multiple I/O channels)</li>\n<li>Es werdne nur benötigte Partitionen gelesen</li>\n<li>Map/Reduce für Partitionen</li>\n<li>Es werden nur benötigte Spalten gelesen</li>\n<li>Q muss mit der Option <code>q -s N</code> für <code>N</code> Threads gestartet werden um parallelle Verarbeitung zu aktivieren</li>\n</ul>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/partitionierung-und-segmentierung-in-kdb/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"4e026cba-c113-4ab7-adf9-8127fe2c0e2f"},"rss:category":[{"@":{},"#":"spaltenorientiert"},{"@":{},"#":"Daten"},{"@":{},"#":"kdb+"},{"@":{},"#":"partitioned"},{"@":{},"#":"segmented"},{"@":{},"#":"BigData"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Fri, 14 Nov 2014 14:00:13 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Tickdaten mit kdb+tick","description":"<p>Wie können 100.000 Ticks/Werte pro Sekunde sicher gespeichert, verteilt und abgefragt werden? Eine Möglichkeit ist das Produkt kdb+tick von <a href=\"http://kx.com/\">KX Systems</a> basierend auf der Programmiersprache <a href=\"http://blog.michaelwittig.info/tag/q/\">q</a> und der Datenbank <a href=\"http://blog.michaelwittig.info/tag/kdb/\">kdb+</a>.</p>\n\n<h2 id=\"architektur\">Architektur</h2>\n\n<p>Neue Ticks/Werte erreichen das System über die Tickerplant (tp) über <code>.u.upd[t;x]</code>.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Nov/kdb_tick-1.png\" alt=\"Architektur kdb+tick\" title=\"\"> </p>\n\n<h3 id=\"tickerplanttp\">Tickerplant (tp)</h3>\n\n<p>Die Tickerplant ist verantwortlich für drei Dinge:</p>\n\n<ol>\n<li>Setzen eines Zeitstempels für den neuen Wert  </li>\n<li>Speichern der Daten in einem Logfile  </li>\n<li>Publush/Subscribe Mechanismus  </li>\n<li>Initiierung des Endofday Events beim wechseln des Tages (00:00 Uhr)</li>\n</ol>\n\n<h4 id=\"speichernderdatenineinemlogfile\">Speichern der Daten in einem Logfile</h4>\n\n<p>Jeder neue Wert wird in einem <a href=\"http://de.wikipedia.org/wiki/WAL-Prinzip\">WAL Verfahren</a> in ein Logfile persistiert. Damit wird verhindert, dass Daten verloren gehen wenn der Prozess abstürzt. Das Logfile wird jeden Tag rotiert.</p>\n\n<h4 id=\"publushsubscribemechanismus\">Publush/Subscribe Mechanismus</h4>\n\n<p>Clients können von Tickerplant asynchrone Updates erhalten wenn sie sich darauf subskribieren. Beim subskribieren erhalten sie die aktuelle Sequenznummer des letzen EIntrags im Logfile.</p>\n\n<h3 id=\"realtimedatenbankrdb\">Realtime Datenbank (rdb)</h3>\n\n<p>Die Realtime Datenbank subskribiert beim Start alle Daten via <code>.u.sub[`;`]</code> bei der Tickerplant und liest das aktuelle Logfile bis zur jeweiligen Sequenznummer ein. Die rdb hält alle Daten des aktuellen Tages im speicher zur Verfügung sortiert nach Zeitstempel.</p>\n\n<h4 id=\"endofday\">Endofday</h4>\n\n<p>Erhält die rdb das Endofday Event <code>.u.end[d]</code> von der tp werden alle Daten des Tages über die Funktion <a href=\"http://code.kx.com/wiki/DotQ/DotQDothdpf\"><code>.Q.hdpf[...]</code></a> auf Platte persistiert. Am Ende benachrichtigt die rdb die hdb das neue Daten vorliegen über die Reload Funktion <code>\\l .</code>.</p>\n\n<h3 id=\"historicaldatenbankhdb\">Historical Datenbank (hdb)</h3>\n\n<p>Die historische Datenbank beantwortet Anfragen auf alle Daten die älter als der aktuelle Tag sind.</p>\n\n<h2 id=\"ntzlichelinks\">Nützliche Links</h2>\n\n<ul>\n<li><a href=\"http://www.kx.com/q/d/tick.htm\">http://www.kx.com/q/d/tick.htm</a></li>\n</ul>","summary":"<p>Wie können 100.000 Ticks/Werte pro Sekunde sicher gespeichert, verteilt und abgefragt werden? Eine Möglichkeit ist das Produkt kdb+tick von <a href=\"http://kx.com/\">KX Systems</a> basierend auf der Programmiersprache <a href=\"http://blog.michaelwittig.info/tag/q/\">q</a> und der Datenbank <a href=\"http://blog.michaelwittig.info/tag/kdb/\">kdb+</a>.</p>\n\n<h2 id=\"architektur\">Architektur</h2>\n\n<p>Neue Ticks/Werte erreichen das System über die Tickerplant (tp) über <code>.u.upd[t;x]</code>.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Nov/kdb_tick-1.png\" alt=\"Architektur kdb+tick\" title=\"\"> </p>\n\n<h3 id=\"tickerplanttp\">Tickerplant (tp)</h3>\n\n<p>Die Tickerplant ist verantwortlich für drei Dinge:</p>\n\n<ol>\n<li>Setzen eines Zeitstempels für den neuen Wert  </li>\n<li>Speichern der Daten in einem Logfile  </li>\n<li>Publush/Subscribe Mechanismus  </li>\n<li>Initiierung des Endofday Events beim wechseln des Tages (00:00 Uhr)</li>\n</ol>\n\n<h4 id=\"speichernderdatenineinemlogfile\">Speichern der Daten in einem Logfile</h4>\n\n<p>Jeder neue Wert wird in einem <a href=\"http://de.wikipedia.org/wiki/WAL-Prinzip\">WAL Verfahren</a> in ein Logfile persistiert. Damit wird verhindert, dass Daten verloren gehen wenn der Prozess abstürzt. Das Logfile wird jeden Tag rotiert.</p>\n\n<h4 id=\"publushsubscribemechanismus\">Publush/Subscribe Mechanismus</h4>\n\n<p>Clients können von Tickerplant asynchrone Updates erhalten wenn sie sich darauf subskribieren. Beim subskribieren erhalten sie die aktuelle Sequenznummer des letzen EIntrags im Logfile.</p>\n\n<h3 id=\"realtimedatenbankrdb\">Realtime Datenbank (rdb)</h3>\n\n<p>Die Realtime Datenbank subskribiert beim Start alle Daten via <code>.u.sub[`;`]</code> bei der Tickerplant und liest das aktuelle Logfile bis zur jeweiligen Sequenznummer ein. Die rdb hält alle Daten des aktuellen Tages im speicher zur Verfügung sortiert nach Zeitstempel.</p>\n\n<h4 id=\"endofday\">Endofday</h4>\n\n<p>Erhält die rdb das Endofday Event <code>.u.end[d]</code> von der tp werden alle Daten des Tages über die Funktion <a href=\"http://code.kx.com/wiki/DotQ/DotQDothdpf\"><code>.Q.hdpf[...]</code></a> auf Platte persistiert. Am Ende benachrichtigt die rdb die hdb das neue Daten vorliegen über die Reload Funktion <code>\\l .</code>.</p>\n\n<h3 id=\"historicaldatenbankhdb\">Historical Datenbank (hdb)</h3>\n\n<p>Die historische Datenbank beantwortet Anfragen auf alle Daten die älter als der aktuelle Tag sind.</p>\n\n<h2 id=\"ntzlichelinks\">Nützliche Links</h2>\n\n<ul>\n<li><a href=\"http://www.kx.com/q/d/tick.htm\">http://www.kx.com/q/d/tick.htm</a></li>\n</ul>","date":"2014-11-13T10:38:34.000Z","pubdate":"2014-11-13T10:38:34.000Z","pubDate":"2014-11-13T10:38:34.000Z","link":"http://blog.michaelwittig.info/tickdaten-mit-kdbtick/","guid":"3ce08427-f370-4fe1-ba47-1dfae69ee179","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["Daten","Q","kdb+","BigData","kdb+tick"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Tickdaten mit kdb+tick"},"rss:description":{"@":{},"#":"<p>Wie können 100.000 Ticks/Werte pro Sekunde sicher gespeichert, verteilt und abgefragt werden? Eine Möglichkeit ist das Produkt kdb+tick von <a href=\"http://kx.com/\">KX Systems</a> basierend auf der Programmiersprache <a href=\"http://blog.michaelwittig.info/tag/q/\">q</a> und der Datenbank <a href=\"http://blog.michaelwittig.info/tag/kdb/\">kdb+</a>.</p>\n\n<h2 id=\"architektur\">Architektur</h2>\n\n<p>Neue Ticks/Werte erreichen das System über die Tickerplant (tp) über <code>.u.upd[t;x]</code>.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Nov/kdb_tick-1.png\" alt=\"Architektur kdb+tick\" title=\"\"> </p>\n\n<h3 id=\"tickerplanttp\">Tickerplant (tp)</h3>\n\n<p>Die Tickerplant ist verantwortlich für drei Dinge:</p>\n\n<ol>\n<li>Setzen eines Zeitstempels für den neuen Wert  </li>\n<li>Speichern der Daten in einem Logfile  </li>\n<li>Publush/Subscribe Mechanismus  </li>\n<li>Initiierung des Endofday Events beim wechseln des Tages (00:00 Uhr)</li>\n</ol>\n\n<h4 id=\"speichernderdatenineinemlogfile\">Speichern der Daten in einem Logfile</h4>\n\n<p>Jeder neue Wert wird in einem <a href=\"http://de.wikipedia.org/wiki/WAL-Prinzip\">WAL Verfahren</a> in ein Logfile persistiert. Damit wird verhindert, dass Daten verloren gehen wenn der Prozess abstürzt. Das Logfile wird jeden Tag rotiert.</p>\n\n<h4 id=\"publushsubscribemechanismus\">Publush/Subscribe Mechanismus</h4>\n\n<p>Clients können von Tickerplant asynchrone Updates erhalten wenn sie sich darauf subskribieren. Beim subskribieren erhalten sie die aktuelle Sequenznummer des letzen EIntrags im Logfile.</p>\n\n<h3 id=\"realtimedatenbankrdb\">Realtime Datenbank (rdb)</h3>\n\n<p>Die Realtime Datenbank subskribiert beim Start alle Daten via <code>.u.sub[`;`]</code> bei der Tickerplant und liest das aktuelle Logfile bis zur jeweiligen Sequenznummer ein. Die rdb hält alle Daten des aktuellen Tages im speicher zur Verfügung sortiert nach Zeitstempel.</p>\n\n<h4 id=\"endofday\">Endofday</h4>\n\n<p>Erhält die rdb das Endofday Event <code>.u.end[d]</code> von der tp werden alle Daten des Tages über die Funktion <a href=\"http://code.kx.com/wiki/DotQ/DotQDothdpf\"><code>.Q.hdpf[...]</code></a> auf Platte persistiert. Am Ende benachrichtigt die rdb die hdb das neue Daten vorliegen über die Reload Funktion <code>\\l .</code>.</p>\n\n<h3 id=\"historicaldatenbankhdb\">Historical Datenbank (hdb)</h3>\n\n<p>Die historische Datenbank beantwortet Anfragen auf alle Daten die älter als der aktuelle Tag sind.</p>\n\n<h2 id=\"ntzlichelinks\">Nützliche Links</h2>\n\n<ul>\n<li><a href=\"http://www.kx.com/q/d/tick.htm\">http://www.kx.com/q/d/tick.htm</a></li>\n</ul>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/tickdaten-mit-kdbtick/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"3ce08427-f370-4fe1-ba47-1dfae69ee179"},"rss:category":[{"@":{},"#":"Daten"},{"@":{},"#":"Q"},{"@":{},"#":"kdb+"},{"@":{},"#":"BigData"},{"@":{},"#":"kdb+tick"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Thu, 13 Nov 2014 10:38:34 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Strings verkürzen mit Node.js","description":"<p>Du kannst jede Zeichenkette die aus <code>N</code> verschiedenen Zeichen besteht in eine Zeichenkette überführen, die aus <code>N+M</code> Zeichen besteht ohne Information zu verlieren, die aber kürzer ist. So wird aus der Zeichnekette:</p>\n\n<p><code>\"-122343.45,94323443,6343.08,934762.3,-8275234,221302234322,-53490269003\"</code> die Zeichnekette:</p>\n\n<p><code>\"7Nx83dblqizc4dGdQI0GVgD9XEcan93Fy438Ogz8an3Bwo903\"</code> welche 30% kürzer ist!</p>\n\n<h2 id=\"beispiel\">Beispiel</h2>\n\n<p>Deine original Zeichnekette besteht aus den Zeichen <code>0</code> und <code>1</code> und lautet: <code>00011011</code>. <br>\nWir wollen die original Zeichenkette in die Zeichen <code>a</code>, <code>b</code>, <code>c</code> und <code>d</code> überführen. <br>\nDazu können wir folgende Tabelle nutzen:</p>\n\n<pre><code>00 =&gt; a\n01 =&gt; b\n10 =&gt; c\n11 =&gt; d\n</code></pre>\n\n<p>Wir überfühen immer zwei Zeichen aus der original Zeichnekette in ein Zeichen der überführten Zeichenkette. Die überführte Zeichenkette lautet <code>abcd</code> und ist damit 50% kürzer als die original Zeichenkette. Je größer der Unterschied zwischen den erlaubten Zeichen, desto (relativ) kürzer wird die überführte Zeichenkette. An Hand der Tabelle kann aus der überführten Zeichenkette wieder die original Zeichenkette hergestellt werden.</p>\n\n<h2 id=\"implementierung\">Implementierung</h2>\n\n<p>Eine Möglichkeit ist die Zeichenkette als eine Folge von Bits zu betrachten. Für die originale Zeichenkette werden weniger Bits pro Zeichen benötigt als für die überführte Zeichenekette. So kann die überführte Zeichenekette erstellt werden, indem man die für die zur Überführung bereitstehenden Zeichen nötigen Bits zu lesen. <br>\nZum Beispiel brauchen wir für die originale Zeichenkette (bei 2 verschiedenen Zeichen) jeweils ein Bit pro Zeichen: <br>\n<code>0 0 0 1 1 0 1 1</code></p>\n\n<p>Für die überführte Zeichenkette (bei 4 verschiedenen Zeichen) jeweils 2 Bits pro Zeichen: <br>\n<code>00 01 10 11</code></p>\n\n<p>In Node.js kann man zum Beispiel mit dem Modul <a href=\"https://www.npmjs.org/package/bit-buffer\">bit-buffer</a> einfach mit Bits arbeiten.</p>\n\n<h2 id=\"shortstr\">shortstr</h2>\n\n<p>Das Ergebnis ist das Modul <a href=\"https://www.npmjs.org/package/shortstr\">shortstr</a> mit dem man einfach Zeichen aus einem Vorrat A in einen Vorrat B überführen kann.</p>\n\n<pre><code>var shortstr = require(\"shortstr\");\n</code></pre>\n\n<p>Erzeuge ein shortener der vom Zeichenvorrat <code>\"01\"</code> auf <code>\"abcd\"</code> mappt mit einer maximalen Input Länge von 8.</p>\n\n<pre><code>var shortener = shortstr.create(\"01\", \"abcd\", 8); // create shortener\n\nshortener.shorten(\"00011011\"); // shorten die originale Zeichenkette\n// =&gt; \"acacbd\" or similar\n\nshortener.expand(\"acacbd\"); // expand die überführte Zeichenkette in die original Zeichenkette\n// =&gt; \"00011011\"\n</code></pre>","summary":"<p>Du kannst jede Zeichenkette die aus <code>N</code> verschiedenen Zeichen besteht in eine Zeichenkette überführen, die aus <code>N+M</code> Zeichen besteht ohne Information zu verlieren, die aber kürzer ist. So wird aus der Zeichnekette:</p>\n\n<p><code>\"-122343.45,94323443,6343.08,934762.3,-8275234,221302234322,-53490269003\"</code> die Zeichnekette:</p>\n\n<p><code>\"7Nx83dblqizc4dGdQI0GVgD9XEcan93Fy438Ogz8an3Bwo903\"</code> welche 30% kürzer ist!</p>\n\n<h2 id=\"beispiel\">Beispiel</h2>\n\n<p>Deine original Zeichnekette besteht aus den Zeichen <code>0</code> und <code>1</code> und lautet: <code>00011011</code>. <br>\nWir wollen die original Zeichenkette in die Zeichen <code>a</code>, <code>b</code>, <code>c</code> und <code>d</code> überführen. <br>\nDazu können wir folgende Tabelle nutzen:</p>\n\n<pre><code>00 =&gt; a\n01 =&gt; b\n10 =&gt; c\n11 =&gt; d\n</code></pre>\n\n<p>Wir überfühen immer zwei Zeichen aus der original Zeichnekette in ein Zeichen der überführten Zeichenkette. Die überführte Zeichenkette lautet <code>abcd</code> und ist damit 50% kürzer als die original Zeichenkette. Je größer der Unterschied zwischen den erlaubten Zeichen, desto (relativ) kürzer wird die überführte Zeichenkette. An Hand der Tabelle kann aus der überführten Zeichenkette wieder die original Zeichenkette hergestellt werden.</p>\n\n<h2 id=\"implementierung\">Implementierung</h2>\n\n<p>Eine Möglichkeit ist die Zeichenkette als eine Folge von Bits zu betrachten. Für die originale Zeichenkette werden weniger Bits pro Zeichen benötigt als für die überführte Zeichenekette. So kann die überführte Zeichenekette erstellt werden, indem man die für die zur Überführung bereitstehenden Zeichen nötigen Bits zu lesen. <br>\nZum Beispiel brauchen wir für die originale Zeichenkette (bei 2 verschiedenen Zeichen) jeweils ein Bit pro Zeichen: <br>\n<code>0 0 0 1 1 0 1 1</code></p>\n\n<p>Für die überführte Zeichenkette (bei 4 verschiedenen Zeichen) jeweils 2 Bits pro Zeichen: <br>\n<code>00 01 10 11</code></p>\n\n<p>In Node.js kann man zum Beispiel mit dem Modul <a href=\"https://www.npmjs.org/package/bit-buffer\">bit-buffer</a> einfach mit Bits arbeiten.</p>\n\n<h2 id=\"shortstr\">shortstr</h2>\n\n<p>Das Ergebnis ist das Modul <a href=\"https://www.npmjs.org/package/shortstr\">shortstr</a> mit dem man einfach Zeichen aus einem Vorrat A in einen Vorrat B überführen kann.</p>\n\n<pre><code>var shortstr = require(\"shortstr\");\n</code></pre>\n\n<p>Erzeuge ein shortener der vom Zeichenvorrat <code>\"01\"</code> auf <code>\"abcd\"</code> mappt mit einer maximalen Input Länge von 8.</p>\n\n<pre><code>var shortener = shortstr.create(\"01\", \"abcd\", 8); // create shortener\n\nshortener.shorten(\"00011011\"); // shorten die originale Zeichenkette\n// =&gt; \"acacbd\" or similar\n\nshortener.expand(\"acacbd\"); // expand die überführte Zeichenkette in die original Zeichenkette\n// =&gt; \"00011011\"\n</code></pre>","date":"2014-11-09T13:49:11.000Z","pubdate":"2014-11-09T13:49:11.000Z","pubDate":"2014-11-09T13:49:11.000Z","link":"http://blog.michaelwittig.info/strings-verkurzen-mit-node-js/","guid":"d4ad16fe-06be-4aa5-8873-78d1f456fc37","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["Node.js"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Strings verkürzen mit Node.js"},"rss:description":{"@":{},"#":"<p>Du kannst jede Zeichenkette die aus <code>N</code> verschiedenen Zeichen besteht in eine Zeichenkette überführen, die aus <code>N+M</code> Zeichen besteht ohne Information zu verlieren, die aber kürzer ist. So wird aus der Zeichnekette:</p>\n\n<p><code>\"-122343.45,94323443,6343.08,934762.3,-8275234,221302234322,-53490269003\"</code> die Zeichnekette:</p>\n\n<p><code>\"7Nx83dblqizc4dGdQI0GVgD9XEcan93Fy438Ogz8an3Bwo903\"</code> welche 30% kürzer ist!</p>\n\n<h2 id=\"beispiel\">Beispiel</h2>\n\n<p>Deine original Zeichnekette besteht aus den Zeichen <code>0</code> und <code>1</code> und lautet: <code>00011011</code>. <br>\nWir wollen die original Zeichenkette in die Zeichen <code>a</code>, <code>b</code>, <code>c</code> und <code>d</code> überführen. <br>\nDazu können wir folgende Tabelle nutzen:</p>\n\n<pre><code>00 =&gt; a\n01 =&gt; b\n10 =&gt; c\n11 =&gt; d\n</code></pre>\n\n<p>Wir überfühen immer zwei Zeichen aus der original Zeichnekette in ein Zeichen der überführten Zeichenkette. Die überführte Zeichenkette lautet <code>abcd</code> und ist damit 50% kürzer als die original Zeichenkette. Je größer der Unterschied zwischen den erlaubten Zeichen, desto (relativ) kürzer wird die überführte Zeichenkette. An Hand der Tabelle kann aus der überführten Zeichenkette wieder die original Zeichenkette hergestellt werden.</p>\n\n<h2 id=\"implementierung\">Implementierung</h2>\n\n<p>Eine Möglichkeit ist die Zeichenkette als eine Folge von Bits zu betrachten. Für die originale Zeichenkette werden weniger Bits pro Zeichen benötigt als für die überführte Zeichenekette. So kann die überführte Zeichenekette erstellt werden, indem man die für die zur Überführung bereitstehenden Zeichen nötigen Bits zu lesen. <br>\nZum Beispiel brauchen wir für die originale Zeichenkette (bei 2 verschiedenen Zeichen) jeweils ein Bit pro Zeichen: <br>\n<code>0 0 0 1 1 0 1 1</code></p>\n\n<p>Für die überführte Zeichenkette (bei 4 verschiedenen Zeichen) jeweils 2 Bits pro Zeichen: <br>\n<code>00 01 10 11</code></p>\n\n<p>In Node.js kann man zum Beispiel mit dem Modul <a href=\"https://www.npmjs.org/package/bit-buffer\">bit-buffer</a> einfach mit Bits arbeiten.</p>\n\n<h2 id=\"shortstr\">shortstr</h2>\n\n<p>Das Ergebnis ist das Modul <a href=\"https://www.npmjs.org/package/shortstr\">shortstr</a> mit dem man einfach Zeichen aus einem Vorrat A in einen Vorrat B überführen kann.</p>\n\n<pre><code>var shortstr = require(\"shortstr\");\n</code></pre>\n\n<p>Erzeuge ein shortener der vom Zeichenvorrat <code>\"01\"</code> auf <code>\"abcd\"</code> mappt mit einer maximalen Input Länge von 8.</p>\n\n<pre><code>var shortener = shortstr.create(\"01\", \"abcd\", 8); // create shortener\n\nshortener.shorten(\"00011011\"); // shorten die originale Zeichenkette\n// =&gt; \"acacbd\" or similar\n\nshortener.expand(\"acacbd\"); // expand die überführte Zeichenkette in die original Zeichenkette\n// =&gt; \"00011011\"\n</code></pre>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/strings-verkurzen-mit-node-js/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"d4ad16fe-06be-4aa5-8873-78d1f456fc37"},"rss:category":{"@":{},"#":"Node.js"},"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Sun, 09 Nov 2014 13:49:11 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Q interfacing with Node.js","description":"<h1 id=\"nodeq\">node-q</h1>\n\n<p>Mit dem NPM <a href=\"https://github.com/cinovo/node-q\">node-q</a> kannst du einfach von <a href=\"http://nodejs.org/\">Node.js</a> mit einem <a href=\"http://kx.com/software.php\">Q</a> Prozess kommunizieren.</p>\n\n<p>Das NPM installierts du mit:</p>\n\n<pre><code>npm install node-q\n</code></pre>\n\n<p>Wenn auf localhost:5000 ein Q Prozess läuft sieht das ganze so aus:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.k(\"sum\", [1, 2, 3], function(err, res) {\n           if (err) throw err;\n        console.log(\"result\", res);\n        con.close(function() {\n            console.log(\"con closed\");\n        });\n    });\n});\n</code></pre>\n\n<p>Dabei wird die Funktion <code>sum</code> mit dem Prameter <code>[1, 2, 3]</code> aufgerufen. <code>res</code> ist dann <code>6</code>.</p>\n\n<h2 id=\"kdb\">kdb+</h2>\n\n<p>Eine Abfrage an die kdb+ Datenbank machst du zum Beispiel so:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.k(\"select price, size from trade where date=2014.10.10,sym=`DAI\", function(err, res) {\n        if (err) throw err;\n        console.log(\"result\", res);\n        con.close(function() {\n            console.log(\"con closed\");\n        });\n    });\n});\n</code></pre>\n\n<p>Beachte, dass <code>res</code> ein Array von Objects ist (siehe kdb+tick Abschnitt).</p>\n\n<h2 id=\"kdbtick\">kdb+tick</h2>\n\n<p>Um dich bei einem kdb+tick Tickerplant Prozess zu subskribieren:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.on(\"upd\", function(table, data) {\n        console.log(table, data);\n    });\n    con.ks(\".u.sub[`;`]\", function(err) {\n        if (err) throw err;\n    });\n});\n</code></pre>\n\n<p>Beachte, dass <code>data</code> ein Array von Objects ist. Wenn deine Tabelle die Spalten utctime, sym, price, size hat, dann sieht <code>data</code> so aus:</p>\n\n<pre><code>[{\n    \"utctime\": `Date`,\n    \"sym\": \"DAI\",\n    \"price: 60.02,\n    \"size\": 1000\n},{\n    \"utctime\": `Date`,\n    \"sym\": \"BMW\",\n    \"price: 82.70,\n    \"size\": 1000\n},]\n</code></pre>\n\n<h2 id=\"limitierungen\">Limitierungen</h2>\n\n<ul>\n<li>Node.js benutzt intern <code>Buffer</code>, <a href=\"http://kx.com/q/c/c.js\">c.js</a> benutzt <code>ArrayBuffer</code> daher wird im Moment zwischen diesen Typen konvertiert.</li>\n<li><del>Es kann nur ein synchroner Request gleichzeitig gemacht werden.</del> (behoben in Version 0.5.0)</li>\n</ul>","summary":"<h1 id=\"nodeq\">node-q</h1>\n\n<p>Mit dem NPM <a href=\"https://github.com/cinovo/node-q\">node-q</a> kannst du einfach von <a href=\"http://nodejs.org/\">Node.js</a> mit einem <a href=\"http://kx.com/software.php\">Q</a> Prozess kommunizieren.</p>\n\n<p>Das NPM installierts du mit:</p>\n\n<pre><code>npm install node-q\n</code></pre>\n\n<p>Wenn auf localhost:5000 ein Q Prozess läuft sieht das ganze so aus:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.k(\"sum\", [1, 2, 3], function(err, res) {\n           if (err) throw err;\n        console.log(\"result\", res);\n        con.close(function() {\n            console.log(\"con closed\");\n        });\n    });\n});\n</code></pre>\n\n<p>Dabei wird die Funktion <code>sum</code> mit dem Prameter <code>[1, 2, 3]</code> aufgerufen. <code>res</code> ist dann <code>6</code>.</p>\n\n<h2 id=\"kdb\">kdb+</h2>\n\n<p>Eine Abfrage an die kdb+ Datenbank machst du zum Beispiel so:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.k(\"select price, size from trade where date=2014.10.10,sym=`DAI\", function(err, res) {\n        if (err) throw err;\n        console.log(\"result\", res);\n        con.close(function() {\n            console.log(\"con closed\");\n        });\n    });\n});\n</code></pre>\n\n<p>Beachte, dass <code>res</code> ein Array von Objects ist (siehe kdb+tick Abschnitt).</p>\n\n<h2 id=\"kdbtick\">kdb+tick</h2>\n\n<p>Um dich bei einem kdb+tick Tickerplant Prozess zu subskribieren:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.on(\"upd\", function(table, data) {\n        console.log(table, data);\n    });\n    con.ks(\".u.sub[`;`]\", function(err) {\n        if (err) throw err;\n    });\n});\n</code></pre>\n\n<p>Beachte, dass <code>data</code> ein Array von Objects ist. Wenn deine Tabelle die Spalten utctime, sym, price, size hat, dann sieht <code>data</code> so aus:</p>\n\n<pre><code>[{\n    \"utctime\": `Date`,\n    \"sym\": \"DAI\",\n    \"price: 60.02,\n    \"size\": 1000\n},{\n    \"utctime\": `Date`,\n    \"sym\": \"BMW\",\n    \"price: 82.70,\n    \"size\": 1000\n},]\n</code></pre>\n\n<h2 id=\"limitierungen\">Limitierungen</h2>\n\n<ul>\n<li>Node.js benutzt intern <code>Buffer</code>, <a href=\"http://kx.com/q/c/c.js\">c.js</a> benutzt <code>ArrayBuffer</code> daher wird im Moment zwischen diesen Typen konvertiert.</li>\n<li><del>Es kann nur ein synchroner Request gleichzeitig gemacht werden.</del> (behoben in Version 0.5.0)</li>\n</ul>","date":"2014-10-23T14:41:33.000Z","pubdate":"2014-10-23T14:41:33.000Z","pubDate":"2014-10-23T14:41:33.000Z","link":"http://blog.michaelwittig.info/q-interfacing-with-node-js/","guid":"f67246db-1e3d-444f-80e9-ec0a41cd9ae5","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["Node.js","Q","kdb+"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Q interfacing with Node.js"},"rss:description":{"@":{},"#":"<h1 id=\"nodeq\">node-q</h1>\n\n<p>Mit dem NPM <a href=\"https://github.com/cinovo/node-q\">node-q</a> kannst du einfach von <a href=\"http://nodejs.org/\">Node.js</a> mit einem <a href=\"http://kx.com/software.php\">Q</a> Prozess kommunizieren.</p>\n\n<p>Das NPM installierts du mit:</p>\n\n<pre><code>npm install node-q\n</code></pre>\n\n<p>Wenn auf localhost:5000 ein Q Prozess läuft sieht das ganze so aus:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.k(\"sum\", [1, 2, 3], function(err, res) {\n           if (err) throw err;\n        console.log(\"result\", res);\n        con.close(function() {\n            console.log(\"con closed\");\n        });\n    });\n});\n</code></pre>\n\n<p>Dabei wird die Funktion <code>sum</code> mit dem Prameter <code>[1, 2, 3]</code> aufgerufen. <code>res</code> ist dann <code>6</code>.</p>\n\n<h2 id=\"kdb\">kdb+</h2>\n\n<p>Eine Abfrage an die kdb+ Datenbank machst du zum Beispiel so:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.k(\"select price, size from trade where date=2014.10.10,sym=`DAI\", function(err, res) {\n        if (err) throw err;\n        console.log(\"result\", res);\n        con.close(function() {\n            console.log(\"con closed\");\n        });\n    });\n});\n</code></pre>\n\n<p>Beachte, dass <code>res</code> ein Array von Objects ist (siehe kdb+tick Abschnitt).</p>\n\n<h2 id=\"kdbtick\">kdb+tick</h2>\n\n<p>Um dich bei einem kdb+tick Tickerplant Prozess zu subskribieren:</p>\n\n<pre><code>var nodeq = require(\"node-q\");\nnodeq.connect(\"localhost\", 5000, function(err, con) {\n    if (err) throw err;\n    console.log(\"connected\");\n    con.on(\"upd\", function(table, data) {\n        console.log(table, data);\n    });\n    con.ks(\".u.sub[`;`]\", function(err) {\n        if (err) throw err;\n    });\n});\n</code></pre>\n\n<p>Beachte, dass <code>data</code> ein Array von Objects ist. Wenn deine Tabelle die Spalten utctime, sym, price, size hat, dann sieht <code>data</code> so aus:</p>\n\n<pre><code>[{\n    \"utctime\": `Date`,\n    \"sym\": \"DAI\",\n    \"price: 60.02,\n    \"size\": 1000\n},{\n    \"utctime\": `Date`,\n    \"sym\": \"BMW\",\n    \"price: 82.70,\n    \"size\": 1000\n},]\n</code></pre>\n\n<h2 id=\"limitierungen\">Limitierungen</h2>\n\n<ul>\n<li>Node.js benutzt intern <code>Buffer</code>, <a href=\"http://kx.com/q/c/c.js\">c.js</a> benutzt <code>ArrayBuffer</code> daher wird im Moment zwischen diesen Typen konvertiert.</li>\n<li><del>Es kann nur ein synchroner Request gleichzeitig gemacht werden.</del> (behoben in Version 0.5.0)</li>\n</ul>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/q-interfacing-with-node-js/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"f67246db-1e3d-444f-80e9-ec0a41cd9ae5"},"rss:category":[{"@":{},"#":"Node.js"},{"@":{},"#":"Q"},{"@":{},"#":"kdb+"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Thu, 23 Oct 2014 14:41:33 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Webseite hosten mit AWS Route 53, S3 und CloudFront","description":"<p>Der Artikel ist <a href=\"http://cloudonauten.de/webseite-hosten-mit-aws-route-53-s3-und-cloudfront/\">umgezogen</a>.</p>","summary":"<p>Der Artikel ist <a href=\"http://cloudonauten.de/webseite-hosten-mit-aws-route-53-s3-und-cloudfront/\">umgezogen</a>.</p>","date":"2014-10-10T05:51:35.000Z","pubdate":"2014-10-10T05:51:35.000Z","pubDate":"2014-10-10T05:51:35.000Z","link":"http://blog.michaelwittig.info/webseite-hosten-mit-aws-route-53-s3-und-cloudfront/","guid":"a7e1dfa9-23f6-4447-9f77-9028d5a129d9","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["Web","AWS","S3","Route 53","CloudFront"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Webseite hosten mit AWS Route 53, S3 und CloudFront"},"rss:description":{"@":{},"#":"<p>Der Artikel ist <a href=\"http://cloudonauten.de/webseite-hosten-mit-aws-route-53-s3-und-cloudfront/\">umgezogen</a>.</p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/webseite-hosten-mit-aws-route-53-s3-und-cloudfront/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"a7e1dfa9-23f6-4447-9f77-9028d5a129d9"},"rss:category":[{"@":{},"#":"Web"},{"@":{},"#":"AWS"},{"@":{},"#":"S3"},{"@":{},"#":"Route 53"},{"@":{},"#":"CloudFront"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Fri, 10 Oct 2014 05:51:35 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Spaltenorientierte Datenbank: Einführung (Teil 1)","description":"<p>Was genau ist eine spaltenorientierte Datenbank und in welchen Fällen kann ich sie einsetzen? Bevor wir diese Fragen beantworten schauen wir uns kurz an wie eine \"normale\" Datenbank funktioniert.</p>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-introduction-part-1-572e5780aebb\">Read this article in english</a></p>\n</blockquote>\n\n<h2 id=\"zeilenorientierteransatz\">zeilenorientierter Ansatz</h2>\n\n<p>Egal ob wir ein Objekt instanziieren oder ein Datensatz (<a href=\"http://de.wikipedia.org/wiki/Tupel_(Informatik)\">Tupel</a>) abspeichern, der Ansatz ist zeilenorientiert.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/zeilenorientiert-1.png\" alt=\"zeilenorientierter Ansatz\"></p>\n\n<p>Als Beispiel Nehmen wir eine Person mit den Eigenschaften Name, Geschlecht und Alter. Unser Datensatz:</p>\n\n<pre><code>Name          Geschlecht     Alter\n==================================\nAlex          männlich       26\nBetina        weiblich       22\nClara         weiblich       23\nDieter        männlich       28\nEmil          männlich       29\nFrederike     weiblich       27\n</code></pre>\n\n<p>Wir werden diese Daten nun im Speicher repräsentiert? Jedes Zeichen steht für 1 Byte also 8 Bit (sehr vereinfacht!)</p>\n\n<pre><code>Stelle 00 |  A l e x \\0 m ä n n l i c h \\0 26\nStelle 15 |  B e t i n a \\0 w e i b l i c h \\0 22\nStelle 32 |  C l a r a \\0 w e i b l i c h \\0 23\nStelle 48 |  D i e t e r \\0 m ä n n l i c h \\0 28\nStelle 65 |  E m i l \\0 m ä n n l i c h \\0 29\nStelle 80 |  F r e d e r i k e \\0 w e i b l i c h \\0 27\n</code></pre>\n\n<p><code>\\0</code> ist das <a href=\"http://de.wikipedia.org/wiki/Nullzeichen\">Nullzeichen</a> um das Ende einer Zeichenkette zu markieren.</p>\n\n<p>Konzentrieren wir uns auf zwei Aspekte: Vorhersagbarkeit und Lokalität.</p>\n\n<h3 id=\"vorhersagbarkeit\">Vorhersagbarkeit</h3>\n\n<p>Mit welcher Sicherheit wissen wir, an welcher Stelle ein neuer Datensatz beginnt und an welcher Stelle welche Eigenschaft zu finden ist? In unserem Beispiel können wir nicht genau vorhersagen an welcher Stelle im Speicher Dieter zu finden ist. Wenn jeder Datensatz eine individuelle Länge hat kann man keine exakten Vorhersagen treffen. Um Dieter zu finden müssen wir den ganzen Speicherbereich durchsuchen</p>\n\n<h3 id=\"lokalitt\">Lokalität</h3>\n\n<p>Wie nahe sind sich zwei Eigenschaften mehrerer Datensätze? Wenn wir das durchschnittliche Alter berechnen wollen müssen wir an Stelle 14, 31, 47, 64 und 79 nachschauen. Das ist insbesondere ein Problem weil wir dadurch die Wahrscheinlichkeit verringern Daten aus dem Cache (Memory) oder aus dem Block (Disk) zu lesen. Wir erschweren es der CPU ebenso vorhersagen zu treffen was der nächste Speicherbereich sein könnte auf den wir zugreifen wollen.</p>\n\n<h3 id=\"ondiskreprsentation\">On-Disk Repräsentation</h3>\n\n<pre><code>Datei Daten.txt\n    Alex;männlich;26\n    Betina;weiblich;22\n    Clara;weiblich;23\n    Dieter;männlich;28\n    Emil;männlich;29\n    Frederike;weiblich;27\n</code></pre>\n\n<p>Um das durchschnittliche Alter zu berechnen müssten wir die Daten <code>Daten.txt</code> lesen und verarbeiten. Dabei interessieren uns eigentlich nur ein Bruchteil der Informationen aus der Datei.</p>\n\n<h2 id=\"spaltenorientierteransatz\">spaltenorientierter Ansatz</h2>\n\n<p>Ein etwas anderer Ansatz ist der spaltenorientierte Ansatz. Das Prinzip ist zu vergleichen mit einem Array für jede Eigenschaft (Spalte).</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/spaltenorientiert.png\" alt=\"spaltenorientierter Ansatz\"></p>\n\n<p>Wie werden unsere Daten spaltenorientiert im Speicher repräsentiert?</p>\n\n<pre><code> Stelle 00 |  A l e x \\0 B e t i n a \\0 C l a r a \\0 D i e t e r \\0 E m i l \\0 F r e d e r i k e \\0\n Stelle 40 |  m ä n n l i c h \\0 w e i b l i c h \\0 w e i b l i c h \\0 m ä n n l i c h \\0 m ä n n l i c h \\0 w e i b l i c h \\0\n Stelle 95 |  26 22 23 28 29 27\n</code></pre>\n\n<h3 id=\"vorhersagbarkeit\">Vorhersagbarkeit</h3>\n\n<p>In unserem Beispiel können wir leider immer noch nicht genau vorhersagen an welcher Stelle im Speicher Dieter zu finden ist. Wenn wir aber Dieter gefunden haben (dieter ist im Array an Index 3) dann können wir durch eine einfache Rechnung herausfinden, wo genau sein Alter zu finden ist: 95 + 3 = 98 => 28. <strong>Bei Datentypen mit einer fixen Länge (Integer, Float, Boolean) haben wir eine sehr gute Vorhersagbarkeit.</strong></p>\n\n<h3 id=\"lokalitt\">Lokalität</h3>\n\n<p>Wenn wir das durchschnittliche Alter berechnen wollen müssen wir jetzt an Stelle 95 - 100 nachschauen. <strong>Die Wahrscheinlichkeit das wir die Daten aus dem Cache oder Block lesen können ist sehr hoch!</strong></p>\n\n<h3 id=\"ondiskreprsentation\">On-Disk Repräsentation</h3>\n\n<p>Die Datensätze könnten wie folgt auf Platte gespeichert werden.</p>\n\n<pre><code>Datei Name.txt\n    Alex\n    Betina\n    Clara\n    Dieter\n    Emil\n    Frederike\nDatei Geschlecht.txt\n    männlich\n    weiblich\n    weiblich\n    männlich\n    männlich\n    weiblich\nDatei Alter.txt\n    26\n    22\n    23\n    28\n    29\n    27\n</code></pre>\n\n<p>Um das durchschnittliche Alter zu berechnen müssten wir nur die Daten <code>Alter.txt</code> lesen und verarbeiten. <strong>Wir lesen nur die Daten, die wir auch wirklich brauchen.</strong></p>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Ein spaltenorientierter Ansatz erhöht die Performance. Es müssen weniger Daten gelesen werden und die Wahrscheinlichkeit für einen Cache-Hit ist höher. Besonders bei Datentypen mit einer fixen Länge können Daten aus mehreren Spalten sehr performant zusammengefügt werden. </p>\n\n<ul>\n<li>TimeSeries.Guru ist eine spaltenorientierte <a href=\"http://www.timeseries.guru/\">Time Series Database as a Service</a></li>\n<li>MonetDB ist eine <a href=\"https://www.monetdb.org/\">Open Source spaltenorientierte Datenbank</a></li>\n<li>Wikipedia Artikel zu <a href=\"http://de.wikipedia.org/wiki/Spaltenorientierte_Datenbank\">Spaltenorientierte Datenbank</a></li>\n</ul>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-introduction-part-1-572e5780aebb\">Read this article in english</a></p>\n</blockquote>","summary":"<p>Was genau ist eine spaltenorientierte Datenbank und in welchen Fällen kann ich sie einsetzen? Bevor wir diese Fragen beantworten schauen wir uns kurz an wie eine \"normale\" Datenbank funktioniert.</p>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-introduction-part-1-572e5780aebb\">Read this article in english</a></p>\n</blockquote>\n\n<h2 id=\"zeilenorientierteransatz\">zeilenorientierter Ansatz</h2>\n\n<p>Egal ob wir ein Objekt instanziieren oder ein Datensatz (<a href=\"http://de.wikipedia.org/wiki/Tupel_(Informatik)\">Tupel</a>) abspeichern, der Ansatz ist zeilenorientiert.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/zeilenorientiert-1.png\" alt=\"zeilenorientierter Ansatz\"></p>\n\n<p>Als Beispiel Nehmen wir eine Person mit den Eigenschaften Name, Geschlecht und Alter. Unser Datensatz:</p>\n\n<pre><code>Name          Geschlecht     Alter\n==================================\nAlex          männlich       26\nBetina        weiblich       22\nClara         weiblich       23\nDieter        männlich       28\nEmil          männlich       29\nFrederike     weiblich       27\n</code></pre>\n\n<p>Wir werden diese Daten nun im Speicher repräsentiert? Jedes Zeichen steht für 1 Byte also 8 Bit (sehr vereinfacht!)</p>\n\n<pre><code>Stelle 00 |  A l e x \\0 m ä n n l i c h \\0 26\nStelle 15 |  B e t i n a \\0 w e i b l i c h \\0 22\nStelle 32 |  C l a r a \\0 w e i b l i c h \\0 23\nStelle 48 |  D i e t e r \\0 m ä n n l i c h \\0 28\nStelle 65 |  E m i l \\0 m ä n n l i c h \\0 29\nStelle 80 |  F r e d e r i k e \\0 w e i b l i c h \\0 27\n</code></pre>\n\n<p><code>\\0</code> ist das <a href=\"http://de.wikipedia.org/wiki/Nullzeichen\">Nullzeichen</a> um das Ende einer Zeichenkette zu markieren.</p>\n\n<p>Konzentrieren wir uns auf zwei Aspekte: Vorhersagbarkeit und Lokalität.</p>\n\n<h3 id=\"vorhersagbarkeit\">Vorhersagbarkeit</h3>\n\n<p>Mit welcher Sicherheit wissen wir, an welcher Stelle ein neuer Datensatz beginnt und an welcher Stelle welche Eigenschaft zu finden ist? In unserem Beispiel können wir nicht genau vorhersagen an welcher Stelle im Speicher Dieter zu finden ist. Wenn jeder Datensatz eine individuelle Länge hat kann man keine exakten Vorhersagen treffen. Um Dieter zu finden müssen wir den ganzen Speicherbereich durchsuchen</p>\n\n<h3 id=\"lokalitt\">Lokalität</h3>\n\n<p>Wie nahe sind sich zwei Eigenschaften mehrerer Datensätze? Wenn wir das durchschnittliche Alter berechnen wollen müssen wir an Stelle 14, 31, 47, 64 und 79 nachschauen. Das ist insbesondere ein Problem weil wir dadurch die Wahrscheinlichkeit verringern Daten aus dem Cache (Memory) oder aus dem Block (Disk) zu lesen. Wir erschweren es der CPU ebenso vorhersagen zu treffen was der nächste Speicherbereich sein könnte auf den wir zugreifen wollen.</p>\n\n<h3 id=\"ondiskreprsentation\">On-Disk Repräsentation</h3>\n\n<pre><code>Datei Daten.txt\n    Alex;männlich;26\n    Betina;weiblich;22\n    Clara;weiblich;23\n    Dieter;männlich;28\n    Emil;männlich;29\n    Frederike;weiblich;27\n</code></pre>\n\n<p>Um das durchschnittliche Alter zu berechnen müssten wir die Daten <code>Daten.txt</code> lesen und verarbeiten. Dabei interessieren uns eigentlich nur ein Bruchteil der Informationen aus der Datei.</p>\n\n<h2 id=\"spaltenorientierteransatz\">spaltenorientierter Ansatz</h2>\n\n<p>Ein etwas anderer Ansatz ist der spaltenorientierte Ansatz. Das Prinzip ist zu vergleichen mit einem Array für jede Eigenschaft (Spalte).</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/spaltenorientiert.png\" alt=\"spaltenorientierter Ansatz\"></p>\n\n<p>Wie werden unsere Daten spaltenorientiert im Speicher repräsentiert?</p>\n\n<pre><code> Stelle 00 |  A l e x \\0 B e t i n a \\0 C l a r a \\0 D i e t e r \\0 E m i l \\0 F r e d e r i k e \\0\n Stelle 40 |  m ä n n l i c h \\0 w e i b l i c h \\0 w e i b l i c h \\0 m ä n n l i c h \\0 m ä n n l i c h \\0 w e i b l i c h \\0\n Stelle 95 |  26 22 23 28 29 27\n</code></pre>\n\n<h3 id=\"vorhersagbarkeit\">Vorhersagbarkeit</h3>\n\n<p>In unserem Beispiel können wir leider immer noch nicht genau vorhersagen an welcher Stelle im Speicher Dieter zu finden ist. Wenn wir aber Dieter gefunden haben (dieter ist im Array an Index 3) dann können wir durch eine einfache Rechnung herausfinden, wo genau sein Alter zu finden ist: 95 + 3 = 98 => 28. <strong>Bei Datentypen mit einer fixen Länge (Integer, Float, Boolean) haben wir eine sehr gute Vorhersagbarkeit.</strong></p>\n\n<h3 id=\"lokalitt\">Lokalität</h3>\n\n<p>Wenn wir das durchschnittliche Alter berechnen wollen müssen wir jetzt an Stelle 95 - 100 nachschauen. <strong>Die Wahrscheinlichkeit das wir die Daten aus dem Cache oder Block lesen können ist sehr hoch!</strong></p>\n\n<h3 id=\"ondiskreprsentation\">On-Disk Repräsentation</h3>\n\n<p>Die Datensätze könnten wie folgt auf Platte gespeichert werden.</p>\n\n<pre><code>Datei Name.txt\n    Alex\n    Betina\n    Clara\n    Dieter\n    Emil\n    Frederike\nDatei Geschlecht.txt\n    männlich\n    weiblich\n    weiblich\n    männlich\n    männlich\n    weiblich\nDatei Alter.txt\n    26\n    22\n    23\n    28\n    29\n    27\n</code></pre>\n\n<p>Um das durchschnittliche Alter zu berechnen müssten wir nur die Daten <code>Alter.txt</code> lesen und verarbeiten. <strong>Wir lesen nur die Daten, die wir auch wirklich brauchen.</strong></p>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Ein spaltenorientierter Ansatz erhöht die Performance. Es müssen weniger Daten gelesen werden und die Wahrscheinlichkeit für einen Cache-Hit ist höher. Besonders bei Datentypen mit einer fixen Länge können Daten aus mehreren Spalten sehr performant zusammengefügt werden. </p>\n\n<ul>\n<li>TimeSeries.Guru ist eine spaltenorientierte <a href=\"http://www.timeseries.guru/\">Time Series Database as a Service</a></li>\n<li>MonetDB ist eine <a href=\"https://www.monetdb.org/\">Open Source spaltenorientierte Datenbank</a></li>\n<li>Wikipedia Artikel zu <a href=\"http://de.wikipedia.org/wiki/Spaltenorientierte_Datenbank\">Spaltenorientierte Datenbank</a></li>\n</ul>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-introduction-part-1-572e5780aebb\">Read this article in english</a></p>\n</blockquote>","date":"2014-09-30T19:07:00.000Z","pubdate":"2014-09-30T19:07:00.000Z","pubDate":"2014-09-30T19:07:00.000Z","link":"http://blog.michaelwittig.info/spaltenorientierte-datenbank-intro-teil-1/","guid":"6dc36fb1-3cb3-4df3-945c-d0e8eb0cb4d7","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["spaltenorientiert","Daten","BigData","Datenbank","Reihe1"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Spaltenorientierte Datenbank: Einführung (Teil 1)"},"rss:description":{"@":{},"#":"<p>Was genau ist eine spaltenorientierte Datenbank und in welchen Fällen kann ich sie einsetzen? Bevor wir diese Fragen beantworten schauen wir uns kurz an wie eine \"normale\" Datenbank funktioniert.</p>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-introduction-part-1-572e5780aebb\">Read this article in english</a></p>\n</blockquote>\n\n<h2 id=\"zeilenorientierteransatz\">zeilenorientierter Ansatz</h2>\n\n<p>Egal ob wir ein Objekt instanziieren oder ein Datensatz (<a href=\"http://de.wikipedia.org/wiki/Tupel_(Informatik)\">Tupel</a>) abspeichern, der Ansatz ist zeilenorientiert.</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/zeilenorientiert-1.png\" alt=\"zeilenorientierter Ansatz\"></p>\n\n<p>Als Beispiel Nehmen wir eine Person mit den Eigenschaften Name, Geschlecht und Alter. Unser Datensatz:</p>\n\n<pre><code>Name          Geschlecht     Alter\n==================================\nAlex          männlich       26\nBetina        weiblich       22\nClara         weiblich       23\nDieter        männlich       28\nEmil          männlich       29\nFrederike     weiblich       27\n</code></pre>\n\n<p>Wir werden diese Daten nun im Speicher repräsentiert? Jedes Zeichen steht für 1 Byte also 8 Bit (sehr vereinfacht!)</p>\n\n<pre><code>Stelle 00 |  A l e x \\0 m ä n n l i c h \\0 26\nStelle 15 |  B e t i n a \\0 w e i b l i c h \\0 22\nStelle 32 |  C l a r a \\0 w e i b l i c h \\0 23\nStelle 48 |  D i e t e r \\0 m ä n n l i c h \\0 28\nStelle 65 |  E m i l \\0 m ä n n l i c h \\0 29\nStelle 80 |  F r e d e r i k e \\0 w e i b l i c h \\0 27\n</code></pre>\n\n<p><code>\\0</code> ist das <a href=\"http://de.wikipedia.org/wiki/Nullzeichen\">Nullzeichen</a> um das Ende einer Zeichenkette zu markieren.</p>\n\n<p>Konzentrieren wir uns auf zwei Aspekte: Vorhersagbarkeit und Lokalität.</p>\n\n<h3 id=\"vorhersagbarkeit\">Vorhersagbarkeit</h3>\n\n<p>Mit welcher Sicherheit wissen wir, an welcher Stelle ein neuer Datensatz beginnt und an welcher Stelle welche Eigenschaft zu finden ist? In unserem Beispiel können wir nicht genau vorhersagen an welcher Stelle im Speicher Dieter zu finden ist. Wenn jeder Datensatz eine individuelle Länge hat kann man keine exakten Vorhersagen treffen. Um Dieter zu finden müssen wir den ganzen Speicherbereich durchsuchen</p>\n\n<h3 id=\"lokalitt\">Lokalität</h3>\n\n<p>Wie nahe sind sich zwei Eigenschaften mehrerer Datensätze? Wenn wir das durchschnittliche Alter berechnen wollen müssen wir an Stelle 14, 31, 47, 64 und 79 nachschauen. Das ist insbesondere ein Problem weil wir dadurch die Wahrscheinlichkeit verringern Daten aus dem Cache (Memory) oder aus dem Block (Disk) zu lesen. Wir erschweren es der CPU ebenso vorhersagen zu treffen was der nächste Speicherbereich sein könnte auf den wir zugreifen wollen.</p>\n\n<h3 id=\"ondiskreprsentation\">On-Disk Repräsentation</h3>\n\n<pre><code>Datei Daten.txt\n    Alex;männlich;26\n    Betina;weiblich;22\n    Clara;weiblich;23\n    Dieter;männlich;28\n    Emil;männlich;29\n    Frederike;weiblich;27\n</code></pre>\n\n<p>Um das durchschnittliche Alter zu berechnen müssten wir die Daten <code>Daten.txt</code> lesen und verarbeiten. Dabei interessieren uns eigentlich nur ein Bruchteil der Informationen aus der Datei.</p>\n\n<h2 id=\"spaltenorientierteransatz\">spaltenorientierter Ansatz</h2>\n\n<p>Ein etwas anderer Ansatz ist der spaltenorientierte Ansatz. Das Prinzip ist zu vergleichen mit einem Array für jede Eigenschaft (Spalte).</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/spaltenorientiert.png\" alt=\"spaltenorientierter Ansatz\"></p>\n\n<p>Wie werden unsere Daten spaltenorientiert im Speicher repräsentiert?</p>\n\n<pre><code> Stelle 00 |  A l e x \\0 B e t i n a \\0 C l a r a \\0 D i e t e r \\0 E m i l \\0 F r e d e r i k e \\0\n Stelle 40 |  m ä n n l i c h \\0 w e i b l i c h \\0 w e i b l i c h \\0 m ä n n l i c h \\0 m ä n n l i c h \\0 w e i b l i c h \\0\n Stelle 95 |  26 22 23 28 29 27\n</code></pre>\n\n<h3 id=\"vorhersagbarkeit\">Vorhersagbarkeit</h3>\n\n<p>In unserem Beispiel können wir leider immer noch nicht genau vorhersagen an welcher Stelle im Speicher Dieter zu finden ist. Wenn wir aber Dieter gefunden haben (dieter ist im Array an Index 3) dann können wir durch eine einfache Rechnung herausfinden, wo genau sein Alter zu finden ist: 95 + 3 = 98 => 28. <strong>Bei Datentypen mit einer fixen Länge (Integer, Float, Boolean) haben wir eine sehr gute Vorhersagbarkeit.</strong></p>\n\n<h3 id=\"lokalitt\">Lokalität</h3>\n\n<p>Wenn wir das durchschnittliche Alter berechnen wollen müssen wir jetzt an Stelle 95 - 100 nachschauen. <strong>Die Wahrscheinlichkeit das wir die Daten aus dem Cache oder Block lesen können ist sehr hoch!</strong></p>\n\n<h3 id=\"ondiskreprsentation\">On-Disk Repräsentation</h3>\n\n<p>Die Datensätze könnten wie folgt auf Platte gespeichert werden.</p>\n\n<pre><code>Datei Name.txt\n    Alex\n    Betina\n    Clara\n    Dieter\n    Emil\n    Frederike\nDatei Geschlecht.txt\n    männlich\n    weiblich\n    weiblich\n    männlich\n    männlich\n    weiblich\nDatei Alter.txt\n    26\n    22\n    23\n    28\n    29\n    27\n</code></pre>\n\n<p>Um das durchschnittliche Alter zu berechnen müssten wir nur die Daten <code>Alter.txt</code> lesen und verarbeiten. <strong>Wir lesen nur die Daten, die wir auch wirklich brauchen.</strong></p>\n\n<h2 id=\"fazit\">Fazit</h2>\n\n<p>Ein spaltenorientierter Ansatz erhöht die Performance. Es müssen weniger Daten gelesen werden und die Wahrscheinlichkeit für einen Cache-Hit ist höher. Besonders bei Datentypen mit einer fixen Länge können Daten aus mehreren Spalten sehr performant zusammengefügt werden. </p>\n\n<ul>\n<li>TimeSeries.Guru ist eine spaltenorientierte <a href=\"http://www.timeseries.guru/\">Time Series Database as a Service</a></li>\n<li>MonetDB ist eine <a href=\"https://www.monetdb.org/\">Open Source spaltenorientierte Datenbank</a></li>\n<li>Wikipedia Artikel zu <a href=\"http://de.wikipedia.org/wiki/Spaltenorientierte_Datenbank\">Spaltenorientierte Datenbank</a></li>\n</ul>\n\n<blockquote>\n  <p>Mehr aus der Reihe <a href=\"http://blog.michaelwittig.info/tag/reihe1/\">Spaltenorientierte Datenbank</a> lesen | <a href=\"https://medium.com/@hellomichibye/column-oriented-database-introduction-part-1-572e5780aebb\">Read this article in english</a></p>\n</blockquote>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/spaltenorientierte-datenbank-intro-teil-1/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"6dc36fb1-3cb3-4df3-945c-d0e8eb0cb4d7"},"rss:category":[{"@":{},"#":"spaltenorientiert"},{"@":{},"#":"Daten"},{"@":{},"#":"BigData"},{"@":{},"#":"Datenbank"},{"@":{},"#":"Reihe1"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Tue, 30 Sep 2014 19:07:00 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Die spaltenorientierte Datenbank kdb+","description":"<p>Im letzten Post wurde <a href=\"http://blog.michaelwittig.info/die-programmiersprache-q/\">die Programmiersprache Q</a> vorgestellt. Mit Q konnte im Handumdrehen eine simple In-Memory Datenbank erstellt werden. In diesem Post geht es um die spaltenorientierte Datenbank kdb+ die Datenbestände auf Festplatte(n) persistieren kann. </p>\n\n<blockquote>\n  <p>kdb+ ist eine Datenbank in der Programmiersprache Q. Die klassische Denkweise: Datenbank + Abfragesprache + Application Server + Anwendungslogik wird mit kdb+ stark verkürzt zu: Datenbank + Anwendungslogik.</p>\n</blockquote>\n\n<h1 id=\"fakten\">Fakten</h1>\n\n<ul>\n<li>Implementiert in Q bzw. K</li>\n<li>“In-Memory Datenbank mit Persistenzschicht” <sup><a href=\"http://blog.michaelwittig.info/#fn1\">1</a></sup></li>\n<li>Daten liegen sowohl im Speicher als auch auf Festplatte im gleichen Format vor</li>\n<li>Spaltenorientiert</li>\n<li>Abfragesprache Q oder Q-SQL</li>\n<li>“Optimierter Zugriff auf Festplatten”</li>\n<li>Vertrieb durch <a href=\"http://kx.com/\">KX Systems</a></li>\n<li>Wird sehr stark in Finanzinstituten eigesetzt</li>\n</ul>\n\n<h1 id=\"waskdbnichtist\">Was kdb+ nicht ist</h1>\n\n<ul>\n<li>Kein Stored Proceudres vs Application Server Abwägungen</li>\n<li>Keine Verluste durch (De)Serialisierung beim Transport zwischen Datenbank ind Applikation durch \"spezielles\" Format</li>\n<li>Keine ORM Probleme</li>\n<li>Keine “universal” Datenbank</li>\n</ul>\n\n<h1 id=\"wiespeichertkdbdatenauffestplatte\">Wie speichert kdb+ Daten auf Festplatte?</h1>\n\n<p>Es gibt generell drei Arten wie Daten persistiert werden können.</p>\n\n<h2 id=\"1einfachemethodemitsetundget\">1. Einfache Methode mit set und get</h2>\n\n<p>Generell kann jede Q Struktur mit <a href=\"http://code.kx.com/wiki/Reference/set\">set</a> in eine Datei gespeichert werden und mit <a href=\"http://code.kx.com/wiki/Reference/get\">get</a> wieder gelesen werden.</p>\n\n<pre><code>`:list set 1 2 3\nget `:list\n</code></pre>\n\n<p>Das ganze funktioniert natürlich auch mit Tabellen.</p>\n\n<pre><code>prices: ([] time:`time$(); sym:`$(); price:`float$())\n`:prices set prices\nget `:prices\n</code></pre>\n\n<p>Sobald der Datensatz größer wird als der zur Verfügung stehende Speicher, können wir diese einfache Methode nicht mehr nutzen. Zur Demonstration erzeugen wir eine ca. 100MB große Tabelle mit 5 Mio Zeilen.</p>\n\n<pre><code>prices: ([] time:5000000#.z.p; sym:5000000#`TEST; price:5000000?100.0)\n`:prices set prices\n</code></pre>\n\n<p>Im <code>q</code> Ordner sollte nun eine Datei prices liegen die ca. 100MB groß ist.</p>\n\n<pre><code>-rw-r--r--   1 michael  staff  105000050 23 Sep 19:44 prices\n</code></pre>\n\n<p>Nun starten wir einen Q Prozess und limitieren diesen auf 50MB Speicher <code>./m32/q -w 50</code>.</p>\n\n<pre><code>get `:prices\n-w abort\n</code></pre>\n\n<p>Der Q Prozess wurde frühzeitig mit der Meldung <code>-w abort</code> beendet, da 50MB Speicher nicht ausreichen um ein 100MB große Datei zu lesen.</p>\n\n<p>Nun stellen wir uns vor die Datei ist keine 100MB sondern 10TB groß und wir stellen schnell fest, dass auch durch den Kauf von mehr Speicher das Problem nicht gelöst werden kann.</p>\n\n<h2 id=\"2splayedtables\">2. Splayed Tables</h2>\n\n<p>Den spaltenorientierten Ansatz konsequent weitergedacht bringt uns dazu, die <code>prices</code> Tabelle auch Spalte für Spalte auf Festplatte zu Speichern. Jede Datei wäre dann nur noch ca 33MB (Vereinfachung!) groß.</p>\n\n<pre><code>./prices/\n    time\n    sym\n    price\n</code></pre>\n\n<p>Eine Weitere Optimierung wäre, dass nicht 50 Mio mal das Symbol <code>`TEST</code> gespeichert wird, sondern eine Datei, die allen Symbolen einen Zahlenwert zuordnet. In kdb+ erreichen wir genau das mit <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/splayed_tables\">splayed tables</a>.</p>\n\n<p>Zuerst wandeln wir die Symbole um:</p>\n\n<pre><code>prices[`sym]: `:db/sym?prices[`sym]\n</code></pre>\n\n<p>Danach benutzen wir wieder <code>set</code> aber mit einem <code>/</code> am Ende.</p>\n\n<pre><code>`:db/prices/ set prices\n</code></pre>\n\n<p>Im <code>q</code> Ordner gibt es jetzt einen <code>db</code> Ordner.</p>\n\n<pre><code>./db/\n    sym\n    ./prices/\n        .d\n        price\n        sym\n        time\n</code></pre>\n\n<p>Nun starten wir wieder einen Q Prozess und limitieren diesen auf 50MB Speicher <code>./m32/q -w 50</code>.</p>\n\n<pre><code>\\l db\nselect sym from prices\n</code></pre>\n\n<p>Der Prozess bleibt am leben, da wir nur eine Spalte der Tabelle gelesen haben und diese passt in unseren Speicher.</p>\n\n<p>Das erstaunliche ist, dass sogar Abfragen, die eigentlich alle Daten benötigen funktionieren.</p>\n\n<pre><code>select last time, entries:count sym, avgprice:avg price from prices\ntime                          entries avgprice\n----------------------------------------------\n2014.09.23D17:44:29.989186000 5000000 49.99852\n</code></pre>\n\n<p>Und genau dafür wird kdb+ sehr geschätzt.</p>\n\n<h2 id=\"3partitionedtables\">3. Partitioned Tables</h2>\n\n<p>Für noch größere Datenmengen brauchen wir eine elegante Möglichkeit, damit unsere Datein nicht zu groß werden. Denkbar wäre hier zum Beispiel eine <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables\">Partitionierung</a> nach Jahr.</p>\n\n<pre><code>./db/\n    sym\n    ./2014/\n      ./prices/\n          .d\n          price\n          sym\n          time\n    ./2013/\n      ./prices/\n          .d\n          price\n          sym\n          time\n</code></pre>\n\n<p>Für noch größere Daten kann Partitionierung nach Tag betrieben werden oder <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4_Segments\">Segmentierung</a>.</p>\n\n<p>Wie Partitionierung und Segmentierung genau funktioniert wird Thema des nächstes Posts.</p>\n\n<p><sup id=\"fn1\">1. Borror (2008): Q For Mortals</sup></p>","summary":"<p>Im letzten Post wurde <a href=\"http://blog.michaelwittig.info/die-programmiersprache-q/\">die Programmiersprache Q</a> vorgestellt. Mit Q konnte im Handumdrehen eine simple In-Memory Datenbank erstellt werden. In diesem Post geht es um die spaltenorientierte Datenbank kdb+ die Datenbestände auf Festplatte(n) persistieren kann. </p>\n\n<blockquote>\n  <p>kdb+ ist eine Datenbank in der Programmiersprache Q. Die klassische Denkweise: Datenbank + Abfragesprache + Application Server + Anwendungslogik wird mit kdb+ stark verkürzt zu: Datenbank + Anwendungslogik.</p>\n</blockquote>\n\n<h1 id=\"fakten\">Fakten</h1>\n\n<ul>\n<li>Implementiert in Q bzw. K</li>\n<li>“In-Memory Datenbank mit Persistenzschicht” <sup><a href=\"http://blog.michaelwittig.info/#fn1\">1</a></sup></li>\n<li>Daten liegen sowohl im Speicher als auch auf Festplatte im gleichen Format vor</li>\n<li>Spaltenorientiert</li>\n<li>Abfragesprache Q oder Q-SQL</li>\n<li>“Optimierter Zugriff auf Festplatten”</li>\n<li>Vertrieb durch <a href=\"http://kx.com/\">KX Systems</a></li>\n<li>Wird sehr stark in Finanzinstituten eigesetzt</li>\n</ul>\n\n<h1 id=\"waskdbnichtist\">Was kdb+ nicht ist</h1>\n\n<ul>\n<li>Kein Stored Proceudres vs Application Server Abwägungen</li>\n<li>Keine Verluste durch (De)Serialisierung beim Transport zwischen Datenbank ind Applikation durch \"spezielles\" Format</li>\n<li>Keine ORM Probleme</li>\n<li>Keine “universal” Datenbank</li>\n</ul>\n\n<h1 id=\"wiespeichertkdbdatenauffestplatte\">Wie speichert kdb+ Daten auf Festplatte?</h1>\n\n<p>Es gibt generell drei Arten wie Daten persistiert werden können.</p>\n\n<h2 id=\"1einfachemethodemitsetundget\">1. Einfache Methode mit set und get</h2>\n\n<p>Generell kann jede Q Struktur mit <a href=\"http://code.kx.com/wiki/Reference/set\">set</a> in eine Datei gespeichert werden und mit <a href=\"http://code.kx.com/wiki/Reference/get\">get</a> wieder gelesen werden.</p>\n\n<pre><code>`:list set 1 2 3\nget `:list\n</code></pre>\n\n<p>Das ganze funktioniert natürlich auch mit Tabellen.</p>\n\n<pre><code>prices: ([] time:`time$(); sym:`$(); price:`float$())\n`:prices set prices\nget `:prices\n</code></pre>\n\n<p>Sobald der Datensatz größer wird als der zur Verfügung stehende Speicher, können wir diese einfache Methode nicht mehr nutzen. Zur Demonstration erzeugen wir eine ca. 100MB große Tabelle mit 5 Mio Zeilen.</p>\n\n<pre><code>prices: ([] time:5000000#.z.p; sym:5000000#`TEST; price:5000000?100.0)\n`:prices set prices\n</code></pre>\n\n<p>Im <code>q</code> Ordner sollte nun eine Datei prices liegen die ca. 100MB groß ist.</p>\n\n<pre><code>-rw-r--r--   1 michael  staff  105000050 23 Sep 19:44 prices\n</code></pre>\n\n<p>Nun starten wir einen Q Prozess und limitieren diesen auf 50MB Speicher <code>./m32/q -w 50</code>.</p>\n\n<pre><code>get `:prices\n-w abort\n</code></pre>\n\n<p>Der Q Prozess wurde frühzeitig mit der Meldung <code>-w abort</code> beendet, da 50MB Speicher nicht ausreichen um ein 100MB große Datei zu lesen.</p>\n\n<p>Nun stellen wir uns vor die Datei ist keine 100MB sondern 10TB groß und wir stellen schnell fest, dass auch durch den Kauf von mehr Speicher das Problem nicht gelöst werden kann.</p>\n\n<h2 id=\"2splayedtables\">2. Splayed Tables</h2>\n\n<p>Den spaltenorientierten Ansatz konsequent weitergedacht bringt uns dazu, die <code>prices</code> Tabelle auch Spalte für Spalte auf Festplatte zu Speichern. Jede Datei wäre dann nur noch ca 33MB (Vereinfachung!) groß.</p>\n\n<pre><code>./prices/\n    time\n    sym\n    price\n</code></pre>\n\n<p>Eine Weitere Optimierung wäre, dass nicht 50 Mio mal das Symbol <code>`TEST</code> gespeichert wird, sondern eine Datei, die allen Symbolen einen Zahlenwert zuordnet. In kdb+ erreichen wir genau das mit <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/splayed_tables\">splayed tables</a>.</p>\n\n<p>Zuerst wandeln wir die Symbole um:</p>\n\n<pre><code>prices[`sym]: `:db/sym?prices[`sym]\n</code></pre>\n\n<p>Danach benutzen wir wieder <code>set</code> aber mit einem <code>/</code> am Ende.</p>\n\n<pre><code>`:db/prices/ set prices\n</code></pre>\n\n<p>Im <code>q</code> Ordner gibt es jetzt einen <code>db</code> Ordner.</p>\n\n<pre><code>./db/\n    sym\n    ./prices/\n        .d\n        price\n        sym\n        time\n</code></pre>\n\n<p>Nun starten wir wieder einen Q Prozess und limitieren diesen auf 50MB Speicher <code>./m32/q -w 50</code>.</p>\n\n<pre><code>\\l db\nselect sym from prices\n</code></pre>\n\n<p>Der Prozess bleibt am leben, da wir nur eine Spalte der Tabelle gelesen haben und diese passt in unseren Speicher.</p>\n\n<p>Das erstaunliche ist, dass sogar Abfragen, die eigentlich alle Daten benötigen funktionieren.</p>\n\n<pre><code>select last time, entries:count sym, avgprice:avg price from prices\ntime                          entries avgprice\n----------------------------------------------\n2014.09.23D17:44:29.989186000 5000000 49.99852\n</code></pre>\n\n<p>Und genau dafür wird kdb+ sehr geschätzt.</p>\n\n<h2 id=\"3partitionedtables\">3. Partitioned Tables</h2>\n\n<p>Für noch größere Datenmengen brauchen wir eine elegante Möglichkeit, damit unsere Datein nicht zu groß werden. Denkbar wäre hier zum Beispiel eine <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables\">Partitionierung</a> nach Jahr.</p>\n\n<pre><code>./db/\n    sym\n    ./2014/\n      ./prices/\n          .d\n          price\n          sym\n          time\n    ./2013/\n      ./prices/\n          .d\n          price\n          sym\n          time\n</code></pre>\n\n<p>Für noch größere Daten kann Partitionierung nach Tag betrieben werden oder <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4_Segments\">Segmentierung</a>.</p>\n\n<p>Wie Partitionierung und Segmentierung genau funktioniert wird Thema des nächstes Posts.</p>\n\n<p><sup id=\"fn1\">1. Borror (2008): Q For Mortals</sup></p>","date":"2014-09-23T18:14:56.000Z","pubdate":"2014-09-23T18:14:56.000Z","pubDate":"2014-09-23T18:14:56.000Z","link":"http://blog.michaelwittig.info/die-spaltenorientierte-datenbank-kdb/","guid":"ecb40e6b-8955-41be-862b-a849f3f2a91f","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["spaltenorientiert","Daten","kdb+","splayed","partitioned","segmented"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Die spaltenorientierte Datenbank kdb+"},"rss:description":{"@":{},"#":"<p>Im letzten Post wurde <a href=\"http://blog.michaelwittig.info/die-programmiersprache-q/\">die Programmiersprache Q</a> vorgestellt. Mit Q konnte im Handumdrehen eine simple In-Memory Datenbank erstellt werden. In diesem Post geht es um die spaltenorientierte Datenbank kdb+ die Datenbestände auf Festplatte(n) persistieren kann. </p>\n\n<blockquote>\n  <p>kdb+ ist eine Datenbank in der Programmiersprache Q. Die klassische Denkweise: Datenbank + Abfragesprache + Application Server + Anwendungslogik wird mit kdb+ stark verkürzt zu: Datenbank + Anwendungslogik.</p>\n</blockquote>\n\n<h1 id=\"fakten\">Fakten</h1>\n\n<ul>\n<li>Implementiert in Q bzw. K</li>\n<li>“In-Memory Datenbank mit Persistenzschicht” <sup><a href=\"http://blog.michaelwittig.info/#fn1\">1</a></sup></li>\n<li>Daten liegen sowohl im Speicher als auch auf Festplatte im gleichen Format vor</li>\n<li>Spaltenorientiert</li>\n<li>Abfragesprache Q oder Q-SQL</li>\n<li>“Optimierter Zugriff auf Festplatten”</li>\n<li>Vertrieb durch <a href=\"http://kx.com/\">KX Systems</a></li>\n<li>Wird sehr stark in Finanzinstituten eigesetzt</li>\n</ul>\n\n<h1 id=\"waskdbnichtist\">Was kdb+ nicht ist</h1>\n\n<ul>\n<li>Kein Stored Proceudres vs Application Server Abwägungen</li>\n<li>Keine Verluste durch (De)Serialisierung beim Transport zwischen Datenbank ind Applikation durch \"spezielles\" Format</li>\n<li>Keine ORM Probleme</li>\n<li>Keine “universal” Datenbank</li>\n</ul>\n\n<h1 id=\"wiespeichertkdbdatenauffestplatte\">Wie speichert kdb+ Daten auf Festplatte?</h1>\n\n<p>Es gibt generell drei Arten wie Daten persistiert werden können.</p>\n\n<h2 id=\"1einfachemethodemitsetundget\">1. Einfache Methode mit set und get</h2>\n\n<p>Generell kann jede Q Struktur mit <a href=\"http://code.kx.com/wiki/Reference/set\">set</a> in eine Datei gespeichert werden und mit <a href=\"http://code.kx.com/wiki/Reference/get\">get</a> wieder gelesen werden.</p>\n\n<pre><code>`:list set 1 2 3\nget `:list\n</code></pre>\n\n<p>Das ganze funktioniert natürlich auch mit Tabellen.</p>\n\n<pre><code>prices: ([] time:`time$(); sym:`$(); price:`float$())\n`:prices set prices\nget `:prices\n</code></pre>\n\n<p>Sobald der Datensatz größer wird als der zur Verfügung stehende Speicher, können wir diese einfache Methode nicht mehr nutzen. Zur Demonstration erzeugen wir eine ca. 100MB große Tabelle mit 5 Mio Zeilen.</p>\n\n<pre><code>prices: ([] time:5000000#.z.p; sym:5000000#`TEST; price:5000000?100.0)\n`:prices set prices\n</code></pre>\n\n<p>Im <code>q</code> Ordner sollte nun eine Datei prices liegen die ca. 100MB groß ist.</p>\n\n<pre><code>-rw-r--r--   1 michael  staff  105000050 23 Sep 19:44 prices\n</code></pre>\n\n<p>Nun starten wir einen Q Prozess und limitieren diesen auf 50MB Speicher <code>./m32/q -w 50</code>.</p>\n\n<pre><code>get `:prices\n-w abort\n</code></pre>\n\n<p>Der Q Prozess wurde frühzeitig mit der Meldung <code>-w abort</code> beendet, da 50MB Speicher nicht ausreichen um ein 100MB große Datei zu lesen.</p>\n\n<p>Nun stellen wir uns vor die Datei ist keine 100MB sondern 10TB groß und wir stellen schnell fest, dass auch durch den Kauf von mehr Speicher das Problem nicht gelöst werden kann.</p>\n\n<h2 id=\"2splayedtables\">2. Splayed Tables</h2>\n\n<p>Den spaltenorientierten Ansatz konsequent weitergedacht bringt uns dazu, die <code>prices</code> Tabelle auch Spalte für Spalte auf Festplatte zu Speichern. Jede Datei wäre dann nur noch ca 33MB (Vereinfachung!) groß.</p>\n\n<pre><code>./prices/\n    time\n    sym\n    price\n</code></pre>\n\n<p>Eine Weitere Optimierung wäre, dass nicht 50 Mio mal das Symbol <code>`TEST</code> gespeichert wird, sondern eine Datei, die allen Symbolen einen Zahlenwert zuordnet. In kdb+ erreichen wir genau das mit <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/splayed_tables\">splayed tables</a>.</p>\n\n<p>Zuerst wandeln wir die Symbole um:</p>\n\n<pre><code>prices[`sym]: `:db/sym?prices[`sym]\n</code></pre>\n\n<p>Danach benutzen wir wieder <code>set</code> aber mit einem <code>/</code> am Ende.</p>\n\n<pre><code>`:db/prices/ set prices\n</code></pre>\n\n<p>Im <code>q</code> Ordner gibt es jetzt einen <code>db</code> Ordner.</p>\n\n<pre><code>./db/\n    sym\n    ./prices/\n        .d\n        price\n        sym\n        time\n</code></pre>\n\n<p>Nun starten wir wieder einen Q Prozess und limitieren diesen auf 50MB Speicher <code>./m32/q -w 50</code>.</p>\n\n<pre><code>\\l db\nselect sym from prices\n</code></pre>\n\n<p>Der Prozess bleibt am leben, da wir nur eine Spalte der Tabelle gelesen haben und diese passt in unseren Speicher.</p>\n\n<p>Das erstaunliche ist, dass sogar Abfragen, die eigentlich alle Daten benötigen funktionieren.</p>\n\n<pre><code>select last time, entries:count sym, avgprice:avg price from prices\ntime                          entries avgprice\n----------------------------------------------\n2014.09.23D17:44:29.989186000 5000000 49.99852\n</code></pre>\n\n<p>Und genau dafür wird kdb+ sehr geschätzt.</p>\n\n<h2 id=\"3partitionedtables\">3. Partitioned Tables</h2>\n\n<p>Für noch größere Datenmengen brauchen wir eine elegante Möglichkeit, damit unsere Datein nicht zu groß werden. Denkbar wäre hier zum Beispiel eine <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables\">Partitionierung</a> nach Jahr.</p>\n\n<pre><code>./db/\n    sym\n    ./2014/\n      ./prices/\n          .d\n          price\n          sym\n          time\n    ./2013/\n      ./prices/\n          .d\n          price\n          sym\n          time\n</code></pre>\n\n<p>Für noch größere Daten kann Partitionierung nach Tag betrieben werden oder <a href=\"http://code.kx.com/wiki/JB:KdbplusForMortals/segments#1.4_Segments\">Segmentierung</a>.</p>\n\n<p>Wie Partitionierung und Segmentierung genau funktioniert wird Thema des nächstes Posts.</p>\n\n<p><sup id=\"fn1\">1. Borror (2008): Q For Mortals</sup></p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/die-spaltenorientierte-datenbank-kdb/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"ecb40e6b-8955-41be-862b-a849f3f2a91f"},"rss:category":[{"@":{},"#":"spaltenorientiert"},{"@":{},"#":"Daten"},{"@":{},"#":"kdb+"},{"@":{},"#":"splayed"},{"@":{},"#":"partitioned"},{"@":{},"#":"segmented"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Tue, 23 Sep 2014 18:14:56 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Die Programmiersprache Q","description":"<p>Die Programmiersprache Q wird hauptsächlich in Finanzinstituten zur Zeitreihenanalyse eingesetzt. Die Sprache bietet uns hochperformante Operationen auf Listen sowie ein sehr einfache aber mächtige Interprozesskommunikation. </p>\n\n<h1 id=\"fakten\">Fakten</h1>\n\n<ul>\n<li>Entwickelt von <a href=\"http://en.wikipedia.org/wiki/Arthur_Whitney_(computer_scientist)\">Arthur Whitney</a></li>\n<li>Beeinflusst von <a href=\"http://en.wikipedia.org/wiki/APL_(programming_language)\">APL</a> (1950er) und <a href=\"http://en.wikipedia.org/wiki/K_(programming_language)\">K</a> (1990er)</li>\n<li>Vektor / Array Processing Language</li>\n<li>Tabellen / spaltenorientiert</li>\n<li>Vertrieb durch <a href=\"http://kx.com/\">KX Systems</a></li>\n<li>Wird sehr stark bei Finanzinstituten eigesetzt</li>\n<li>Hauptsächlich single threaded</li>\n</ul>\n\n<h1 id=\"gettingstarted\">Getting started</h1>\n\n<p>Die 32-bit Version von Q gibt es unter <a href=\"http://kx.com/software-download.php\">http://kx.com/software-download.php</a> kostenlos zum Download (auch für kommerzielle Zwecke). </p>\n\n<p>Nach dem entpacken sollte das ganze so aussehen (je nach Plattform wird aus m32 (Mac OS X) ein l32 (Linux) oder w32 (Windows): <br>\n<img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/q_files.png\" alt=\"Ordnerstruktur\"></p>\n\n<p>Aus dem <code>q</code> Ordner heraus startest du eine Q Konsole mit <code>./m32/q</code> und landest dann hier:</p>\n\n<pre><code>KDB+ 3.1 2014.08.22 Copyright (C) 1993-2014 Kx Systems\nm32/ 4()core 8192MB michael mwittig.fritz.box 192.168.178.20 NONEXPIRE  \n\nWelcome to kdb+ 32bit edition\nFor support please see http://groups.google.com/d/forum/personal-kdbplus\nTutorials can be found at http://code.kx.com/wiki/Tutorials\nTo exit, type \\\\\nTo remove this startup msg, edit q.q\n</code></pre>\n\n<p>Wichtig ist der exit Befehlt <code>\\\\</code> zum beenden der Konsole.</p>\n\n<h1 id=\"syntax\">Syntax</h1>\n\n<pre><code>1 2 3                                       / Liste von longs\n`a`b`c                                      / Symbol Liste\n\nsum 1 2 3 -&gt; 6                              / Alle Funktionen können auch mit Listen arbeiten\n\n4+1 2 3 -&gt; 5 6 7                            / Q wertet von rechts nach links aus\n3 * 4+5 -&gt; 27\n(1 2 3)+(4 5 6) -&gt; 5 7 9\n\n`a`b`c!(1 2 3)                              / Dictionary von Symbolen auf Longs\n\nflip `time`sym`price!(();();())             / Tabellen sind Dictionaries von Symbolen auf Listen\n([] time:(); sym:(); price:())              / Tabellen können auch einfacher erzeugt werden\nselect from t where time&gt;10:00:00,sym=`Daimler  / Abfragesprache Q-SQL\n</code></pre>\n\n<h1 id=\"interprozesskommunikation\">Interprozesskommunikation</h1>\n\n<p>Da Q hauptsächlich single threaded arbeitet ist ein wesentliches Feature Interprozesskommunikation (IPC).</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/q_ipc.png\" alt=\"Q IPC\"></p>\n\n<p>Wir bauen uns nun eine kleine Datenbank: Den Q Prozess (1) für die Datenbank startet wir mit <code>./m32/q -p 8000</code>. <code>-p 8000</code> gibt an, dass der Q Prozess (1) auf den Port 8000 hört. Nun erstellen wir eine Tabelle in dem wir in die Q Konsole (1) eingeben:</p>\n\n<pre><code>prices: ([] time:`time$(); sym:`$(); price:`float$())\n</code></pre>\n\n<p>Wir startet einen zweiten Q Prozess (2) mit <code>./m32/q</code> der sich auf die Datenbank verbinden soll um Daten hinzuzufügen und abzufragen. In der Q Konsole (2) führen wir aus:</p>\n\n<pre><code>h: hopen 8000           / verbindet sich mit dem Prozess unter dem Port 8000\nh \"select from prices\"  / Frägt die Daten ab (sind jetzt noch leer)\nh \"insert[`prices;(10:00:00.000;`Daimler;63.70)]\"   / Fügt Daten hinzu\nh \"insert[`prices;(11:00:00.000;`Daimler;63.60)]\"\nh \"insert[`prices;(12:00:00.000;`Daimler;63.50)]\"\nh \"insert[`prices;(13:00:00.000;`Daimler;63.60)]\"\nh \"select from prices\"  / Frägt die Daten ab (jetzt 4 Datensätze vorhanden)\n</code></pre>\n\n<p>Wir startet einen dritten Q Prozess (3) mit <code>./m32/q</code> In der Q Konsole (3) führen wir aus:</p>\n\n<pre><code>h: hopen 8000           / verbindet sich mit dem Prozess unter dem Port 8000\nh \"select from prices\"  / Frägt die Daten ab (jetzt 4 Datensätze vorhanden)\n</code></pre>\n\n<h1 id=\"fazit\">Fazit</h1>\n\n<p>Wir können jetzt also von den Q Prozessen (2) und (3) auf (1) zugreifen und haben uns dadurch eine kleine Datenbank gebaut. Q eignet sich immer dann, wenn es darum geht Zeitreihen zu speichern und zu analysieren. Vorstellbar sind neben Finanzdaten wie Preise für Aktien auch Messwerte von Sensoren. Q eignet sich nicht für Strings!  </p>","summary":"<p>Die Programmiersprache Q wird hauptsächlich in Finanzinstituten zur Zeitreihenanalyse eingesetzt. Die Sprache bietet uns hochperformante Operationen auf Listen sowie ein sehr einfache aber mächtige Interprozesskommunikation. </p>\n\n<h1 id=\"fakten\">Fakten</h1>\n\n<ul>\n<li>Entwickelt von <a href=\"http://en.wikipedia.org/wiki/Arthur_Whitney_(computer_scientist)\">Arthur Whitney</a></li>\n<li>Beeinflusst von <a href=\"http://en.wikipedia.org/wiki/APL_(programming_language)\">APL</a> (1950er) und <a href=\"http://en.wikipedia.org/wiki/K_(programming_language)\">K</a> (1990er)</li>\n<li>Vektor / Array Processing Language</li>\n<li>Tabellen / spaltenorientiert</li>\n<li>Vertrieb durch <a href=\"http://kx.com/\">KX Systems</a></li>\n<li>Wird sehr stark bei Finanzinstituten eigesetzt</li>\n<li>Hauptsächlich single threaded</li>\n</ul>\n\n<h1 id=\"gettingstarted\">Getting started</h1>\n\n<p>Die 32-bit Version von Q gibt es unter <a href=\"http://kx.com/software-download.php\">http://kx.com/software-download.php</a> kostenlos zum Download (auch für kommerzielle Zwecke). </p>\n\n<p>Nach dem entpacken sollte das ganze so aussehen (je nach Plattform wird aus m32 (Mac OS X) ein l32 (Linux) oder w32 (Windows): <br>\n<img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/q_files.png\" alt=\"Ordnerstruktur\"></p>\n\n<p>Aus dem <code>q</code> Ordner heraus startest du eine Q Konsole mit <code>./m32/q</code> und landest dann hier:</p>\n\n<pre><code>KDB+ 3.1 2014.08.22 Copyright (C) 1993-2014 Kx Systems\nm32/ 4()core 8192MB michael mwittig.fritz.box 192.168.178.20 NONEXPIRE  \n\nWelcome to kdb+ 32bit edition\nFor support please see http://groups.google.com/d/forum/personal-kdbplus\nTutorials can be found at http://code.kx.com/wiki/Tutorials\nTo exit, type \\\\\nTo remove this startup msg, edit q.q\n</code></pre>\n\n<p>Wichtig ist der exit Befehlt <code>\\\\</code> zum beenden der Konsole.</p>\n\n<h1 id=\"syntax\">Syntax</h1>\n\n<pre><code>1 2 3                                       / Liste von longs\n`a`b`c                                      / Symbol Liste\n\nsum 1 2 3 -&gt; 6                              / Alle Funktionen können auch mit Listen arbeiten\n\n4+1 2 3 -&gt; 5 6 7                            / Q wertet von rechts nach links aus\n3 * 4+5 -&gt; 27\n(1 2 3)+(4 5 6) -&gt; 5 7 9\n\n`a`b`c!(1 2 3)                              / Dictionary von Symbolen auf Longs\n\nflip `time`sym`price!(();();())             / Tabellen sind Dictionaries von Symbolen auf Listen\n([] time:(); sym:(); price:())              / Tabellen können auch einfacher erzeugt werden\nselect from t where time&gt;10:00:00,sym=`Daimler  / Abfragesprache Q-SQL\n</code></pre>\n\n<h1 id=\"interprozesskommunikation\">Interprozesskommunikation</h1>\n\n<p>Da Q hauptsächlich single threaded arbeitet ist ein wesentliches Feature Interprozesskommunikation (IPC).</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/q_ipc.png\" alt=\"Q IPC\"></p>\n\n<p>Wir bauen uns nun eine kleine Datenbank: Den Q Prozess (1) für die Datenbank startet wir mit <code>./m32/q -p 8000</code>. <code>-p 8000</code> gibt an, dass der Q Prozess (1) auf den Port 8000 hört. Nun erstellen wir eine Tabelle in dem wir in die Q Konsole (1) eingeben:</p>\n\n<pre><code>prices: ([] time:`time$(); sym:`$(); price:`float$())\n</code></pre>\n\n<p>Wir startet einen zweiten Q Prozess (2) mit <code>./m32/q</code> der sich auf die Datenbank verbinden soll um Daten hinzuzufügen und abzufragen. In der Q Konsole (2) führen wir aus:</p>\n\n<pre><code>h: hopen 8000           / verbindet sich mit dem Prozess unter dem Port 8000\nh \"select from prices\"  / Frägt die Daten ab (sind jetzt noch leer)\nh \"insert[`prices;(10:00:00.000;`Daimler;63.70)]\"   / Fügt Daten hinzu\nh \"insert[`prices;(11:00:00.000;`Daimler;63.60)]\"\nh \"insert[`prices;(12:00:00.000;`Daimler;63.50)]\"\nh \"insert[`prices;(13:00:00.000;`Daimler;63.60)]\"\nh \"select from prices\"  / Frägt die Daten ab (jetzt 4 Datensätze vorhanden)\n</code></pre>\n\n<p>Wir startet einen dritten Q Prozess (3) mit <code>./m32/q</code> In der Q Konsole (3) führen wir aus:</p>\n\n<pre><code>h: hopen 8000           / verbindet sich mit dem Prozess unter dem Port 8000\nh \"select from prices\"  / Frägt die Daten ab (jetzt 4 Datensätze vorhanden)\n</code></pre>\n\n<h1 id=\"fazit\">Fazit</h1>\n\n<p>Wir können jetzt also von den Q Prozessen (2) und (3) auf (1) zugreifen und haben uns dadurch eine kleine Datenbank gebaut. Q eignet sich immer dann, wenn es darum geht Zeitreihen zu speichern und zu analysieren. Vorstellbar sind neben Finanzdaten wie Preise für Aktien auch Messwerte von Sensoren. Q eignet sich nicht für Strings!  </p>","date":"2014-09-20T12:12:15.000Z","pubdate":"2014-09-20T12:12:15.000Z","pubDate":"2014-09-20T12:12:15.000Z","link":"http://blog.michaelwittig.info/die-programmiersprache-q/","guid":"6092044b-e055-420b-99d1-061b620bdfd6","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["spaltenorientiert","Programmierung","Daten","Q"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Die Programmiersprache Q"},"rss:description":{"@":{},"#":"<p>Die Programmiersprache Q wird hauptsächlich in Finanzinstituten zur Zeitreihenanalyse eingesetzt. Die Sprache bietet uns hochperformante Operationen auf Listen sowie ein sehr einfache aber mächtige Interprozesskommunikation. </p>\n\n<h1 id=\"fakten\">Fakten</h1>\n\n<ul>\n<li>Entwickelt von <a href=\"http://en.wikipedia.org/wiki/Arthur_Whitney_(computer_scientist)\">Arthur Whitney</a></li>\n<li>Beeinflusst von <a href=\"http://en.wikipedia.org/wiki/APL_(programming_language)\">APL</a> (1950er) und <a href=\"http://en.wikipedia.org/wiki/K_(programming_language)\">K</a> (1990er)</li>\n<li>Vektor / Array Processing Language</li>\n<li>Tabellen / spaltenorientiert</li>\n<li>Vertrieb durch <a href=\"http://kx.com/\">KX Systems</a></li>\n<li>Wird sehr stark bei Finanzinstituten eigesetzt</li>\n<li>Hauptsächlich single threaded</li>\n</ul>\n\n<h1 id=\"gettingstarted\">Getting started</h1>\n\n<p>Die 32-bit Version von Q gibt es unter <a href=\"http://kx.com/software-download.php\">http://kx.com/software-download.php</a> kostenlos zum Download (auch für kommerzielle Zwecke). </p>\n\n<p>Nach dem entpacken sollte das ganze so aussehen (je nach Plattform wird aus m32 (Mac OS X) ein l32 (Linux) oder w32 (Windows): <br>\n<img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/q_files.png\" alt=\"Ordnerstruktur\"></p>\n\n<p>Aus dem <code>q</code> Ordner heraus startest du eine Q Konsole mit <code>./m32/q</code> und landest dann hier:</p>\n\n<pre><code>KDB+ 3.1 2014.08.22 Copyright (C) 1993-2014 Kx Systems\nm32/ 4()core 8192MB michael mwittig.fritz.box 192.168.178.20 NONEXPIRE  \n\nWelcome to kdb+ 32bit edition\nFor support please see http://groups.google.com/d/forum/personal-kdbplus\nTutorials can be found at http://code.kx.com/wiki/Tutorials\nTo exit, type \\\\\nTo remove this startup msg, edit q.q\n</code></pre>\n\n<p>Wichtig ist der exit Befehlt <code>\\\\</code> zum beenden der Konsole.</p>\n\n<h1 id=\"syntax\">Syntax</h1>\n\n<pre><code>1 2 3                                       / Liste von longs\n`a`b`c                                      / Symbol Liste\n\nsum 1 2 3 -&gt; 6                              / Alle Funktionen können auch mit Listen arbeiten\n\n4+1 2 3 -&gt; 5 6 7                            / Q wertet von rechts nach links aus\n3 * 4+5 -&gt; 27\n(1 2 3)+(4 5 6) -&gt; 5 7 9\n\n`a`b`c!(1 2 3)                              / Dictionary von Symbolen auf Longs\n\nflip `time`sym`price!(();();())             / Tabellen sind Dictionaries von Symbolen auf Listen\n([] time:(); sym:(); price:())              / Tabellen können auch einfacher erzeugt werden\nselect from t where time&gt;10:00:00,sym=`Daimler  / Abfragesprache Q-SQL\n</code></pre>\n\n<h1 id=\"interprozesskommunikation\">Interprozesskommunikation</h1>\n\n<p>Da Q hauptsächlich single threaded arbeitet ist ein wesentliches Feature Interprozesskommunikation (IPC).</p>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/q_ipc.png\" alt=\"Q IPC\"></p>\n\n<p>Wir bauen uns nun eine kleine Datenbank: Den Q Prozess (1) für die Datenbank startet wir mit <code>./m32/q -p 8000</code>. <code>-p 8000</code> gibt an, dass der Q Prozess (1) auf den Port 8000 hört. Nun erstellen wir eine Tabelle in dem wir in die Q Konsole (1) eingeben:</p>\n\n<pre><code>prices: ([] time:`time$(); sym:`$(); price:`float$())\n</code></pre>\n\n<p>Wir startet einen zweiten Q Prozess (2) mit <code>./m32/q</code> der sich auf die Datenbank verbinden soll um Daten hinzuzufügen und abzufragen. In der Q Konsole (2) führen wir aus:</p>\n\n<pre><code>h: hopen 8000           / verbindet sich mit dem Prozess unter dem Port 8000\nh \"select from prices\"  / Frägt die Daten ab (sind jetzt noch leer)\nh \"insert[`prices;(10:00:00.000;`Daimler;63.70)]\"   / Fügt Daten hinzu\nh \"insert[`prices;(11:00:00.000;`Daimler;63.60)]\"\nh \"insert[`prices;(12:00:00.000;`Daimler;63.50)]\"\nh \"insert[`prices;(13:00:00.000;`Daimler;63.60)]\"\nh \"select from prices\"  / Frägt die Daten ab (jetzt 4 Datensätze vorhanden)\n</code></pre>\n\n<p>Wir startet einen dritten Q Prozess (3) mit <code>./m32/q</code> In der Q Konsole (3) führen wir aus:</p>\n\n<pre><code>h: hopen 8000           / verbindet sich mit dem Prozess unter dem Port 8000\nh \"select from prices\"  / Frägt die Daten ab (jetzt 4 Datensätze vorhanden)\n</code></pre>\n\n<h1 id=\"fazit\">Fazit</h1>\n\n<p>Wir können jetzt also von den Q Prozessen (2) und (3) auf (1) zugreifen und haben uns dadurch eine kleine Datenbank gebaut. Q eignet sich immer dann, wenn es darum geht Zeitreihen zu speichern und zu analysieren. Vorstellbar sind neben Finanzdaten wie Preise für Aktien auch Messwerte von Sensoren. Q eignet sich nicht für Strings!  </p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/die-programmiersprache-q/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"6092044b-e055-420b-99d1-061b620bdfd6"},"rss:category":[{"@":{},"#":"spaltenorientiert"},{"@":{},"#":"Programmierung"},{"@":{},"#":"Daten"},{"@":{},"#":"Q"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Sat, 20 Sep 2014 12:12:15 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}},{"title":"Tabellen und JSON","description":"<p>Manche Informationen lassen sich einfach am besten in Tabellen darstellen. Soll der Inhalt der Tabelle dynamisch geladen werden, z. B. durch unendliches Scrollen oder einen Weiter-Button, so muss der Inhalt dynamisch nachgeladen werden. Das <a href=\"http://de.wikipedia.org/wiki/JavaScript_Object_Notation\">JSON-Format</a> bietet sich hierbei an, da es auf dem Browser mit <code>JSON.parse(json)</code> in ein JavaScript Wert umgewandelt werden kann.</p>\n\n<h1 id=\"zeilenorientierteransatz\">zeilenorientierter Ansatz</h1>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/zeilenorientiert-1.png\" alt=\"zeilenorientierter Ansatz\">\nEin zeilenorientierter Ansatz wäre die Daten als Liste von Objekten zu übertragen.</p>\n\n<pre><code>[\n   {\"name\": \"Anne\", \"age\": 25, \"city\": \"Stuttgart\"},\n   {\"name\": \"Mike\", \"age\": 33, \"city\": \"London\"}\n   {\"name\": \"Juli\", \"age\": 27, \"city\": \"Paris\"}\n]\n</code></pre>\n\n<p>Wie in den meisten Datenbanken werden einzelne Einträge als Zeile dargestellt und gespeichert. <br>\nAuffallend ist jedoch, dass sehr viele redundante Informationen vorhanden sind, da wir die Spaltennamen (name, age, city) in jeder Zeile wiederholen.</p>\n\n<p>Können wir in Zeiten von mobilen Endgeräten unseren Nutzern nicht etwas Bandbreite sparen?</p>\n\n<h1 id=\"spaltenorientierteransatz\">spaltenorientierter Ansatz</h1>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/spaltenorientiert.png\" alt=\"spaltenorientierter Ansatz\"></p>\n\n<p>Ein spaltenorientierter Ansatz wäre die Daten als Objekt von Listen zu übertragen.</p>\n\n<pre><code>{\n \"name\": [\"Anne\", \"Mike\", \"Juli\"],\n \"age\": [25, 33, 27],\n \"city\": [\"Stuttgart\", \"London\", \"Paris\"]\n}\n</code></pre>\n\n<p>Die redundanten Informationen wurden eliminiert. Übrig bleibt die minimale Information die für die Tabelle benötigt wird.</p>\n\n<h1 id=\"fliptable\">fliptable</h1>\n\n<p>Das Node.js Modul <a href=\"https://www.npmjs.org/package/fliptable\">fliptable</a> bietet genau diese Funktionalität. Es wandelt zeilenorientierte Tabellen in spaltenorientierte um (und vice versa).</p>\n\n<p>Installieren kannst du das Modul mit</p>\n\n<pre><code>npm install fliptable\n</code></pre>\n\n<p>eine zeilenorientierte Tabelle in eine spaltenorientierte flippen</p>\n\n<pre><code>var fliptable = require(\"fliptable\");\nfliptable([{a: 1}, {a: 2}, {a: 3}])\n=&gt;\n{a: [1, 2, 3]}\n</code></pre>\n\n<p>Funktioniert auch vice versa</p>\n\n<pre><code>fliptable({a: [1, 2, 3]})\n=&gt;\n[{a: 1}, {a: 2}, {a: 3}]\n</code></pre>\n\n<p>Du kannst also die Tabelle auf dem Server vor dem versenden flippen und auf dem Client flippen und hast genau die Tabelle wie ohne <code>fliptable</code> nur 50% kleiner bei der Übertragen zum Client.</p>\n\n<blockquote>\n  <p>Das Node.js Modul <a href=\"https://www.npmjs.org/package/fliptable\">fliptable</a> wurde beim <a href=\"http://www.meetup.com/stuttgartjs/events/181026072/\">Javascript-Meetup Stuttgart</a> am 02.06.2014 vorgestellt.</p>\n</blockquote>\n\n<h1 id=\"fazitspare50\">Fazit: Spare 50%</h1>\n\n<p>Eine spaltenorientierter JSON String dieser <a href=\"https://github.com/michaelwittig/fliptable/blob/master/test/data.json\">Beispieltabelle</a> ist 449.600 Zeichen lang (<code>JSON.stringify(table)</code>). <br>\nMit <code>fliptable(table)</code> ist der JSON String nur 212.382 Zeichen lang.</p>\n\n<p>Du sparst als mehr als 50% ohne Information zu verlieren!</p>","summary":"<p>Manche Informationen lassen sich einfach am besten in Tabellen darstellen. Soll der Inhalt der Tabelle dynamisch geladen werden, z. B. durch unendliches Scrollen oder einen Weiter-Button, so muss der Inhalt dynamisch nachgeladen werden. Das <a href=\"http://de.wikipedia.org/wiki/JavaScript_Object_Notation\">JSON-Format</a> bietet sich hierbei an, da es auf dem Browser mit <code>JSON.parse(json)</code> in ein JavaScript Wert umgewandelt werden kann.</p>\n\n<h1 id=\"zeilenorientierteransatz\">zeilenorientierter Ansatz</h1>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/zeilenorientiert-1.png\" alt=\"zeilenorientierter Ansatz\">\nEin zeilenorientierter Ansatz wäre die Daten als Liste von Objekten zu übertragen.</p>\n\n<pre><code>[\n   {\"name\": \"Anne\", \"age\": 25, \"city\": \"Stuttgart\"},\n   {\"name\": \"Mike\", \"age\": 33, \"city\": \"London\"}\n   {\"name\": \"Juli\", \"age\": 27, \"city\": \"Paris\"}\n]\n</code></pre>\n\n<p>Wie in den meisten Datenbanken werden einzelne Einträge als Zeile dargestellt und gespeichert. <br>\nAuffallend ist jedoch, dass sehr viele redundante Informationen vorhanden sind, da wir die Spaltennamen (name, age, city) in jeder Zeile wiederholen.</p>\n\n<p>Können wir in Zeiten von mobilen Endgeräten unseren Nutzern nicht etwas Bandbreite sparen?</p>\n\n<h1 id=\"spaltenorientierteransatz\">spaltenorientierter Ansatz</h1>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/spaltenorientiert.png\" alt=\"spaltenorientierter Ansatz\"></p>\n\n<p>Ein spaltenorientierter Ansatz wäre die Daten als Objekt von Listen zu übertragen.</p>\n\n<pre><code>{\n \"name\": [\"Anne\", \"Mike\", \"Juli\"],\n \"age\": [25, 33, 27],\n \"city\": [\"Stuttgart\", \"London\", \"Paris\"]\n}\n</code></pre>\n\n<p>Die redundanten Informationen wurden eliminiert. Übrig bleibt die minimale Information die für die Tabelle benötigt wird.</p>\n\n<h1 id=\"fliptable\">fliptable</h1>\n\n<p>Das Node.js Modul <a href=\"https://www.npmjs.org/package/fliptable\">fliptable</a> bietet genau diese Funktionalität. Es wandelt zeilenorientierte Tabellen in spaltenorientierte um (und vice versa).</p>\n\n<p>Installieren kannst du das Modul mit</p>\n\n<pre><code>npm install fliptable\n</code></pre>\n\n<p>eine zeilenorientierte Tabelle in eine spaltenorientierte flippen</p>\n\n<pre><code>var fliptable = require(\"fliptable\");\nfliptable([{a: 1}, {a: 2}, {a: 3}])\n=&gt;\n{a: [1, 2, 3]}\n</code></pre>\n\n<p>Funktioniert auch vice versa</p>\n\n<pre><code>fliptable({a: [1, 2, 3]})\n=&gt;\n[{a: 1}, {a: 2}, {a: 3}]\n</code></pre>\n\n<p>Du kannst also die Tabelle auf dem Server vor dem versenden flippen und auf dem Client flippen und hast genau die Tabelle wie ohne <code>fliptable</code> nur 50% kleiner bei der Übertragen zum Client.</p>\n\n<blockquote>\n  <p>Das Node.js Modul <a href=\"https://www.npmjs.org/package/fliptable\">fliptable</a> wurde beim <a href=\"http://www.meetup.com/stuttgartjs/events/181026072/\">Javascript-Meetup Stuttgart</a> am 02.06.2014 vorgestellt.</p>\n</blockquote>\n\n<h1 id=\"fazitspare50\">Fazit: Spare 50%</h1>\n\n<p>Eine spaltenorientierter JSON String dieser <a href=\"https://github.com/michaelwittig/fliptable/blob/master/test/data.json\">Beispieltabelle</a> ist 449.600 Zeichen lang (<code>JSON.stringify(table)</code>). <br>\nMit <code>fliptable(table)</code> ist der JSON String nur 212.382 Zeichen lang.</p>\n\n<p>Du sparst als mehr als 50% ohne Information zu verlieren!</p>","date":"2014-09-19T20:42:15.000Z","pubdate":"2014-09-19T20:42:15.000Z","pubDate":"2014-09-19T20:42:15.000Z","link":"http://blog.michaelwittig.info/tabellen-und-json/","guid":"4adaab2e-e91a-48cc-868a-bd1308ccc8d4","author":"Michael","comments":null,"origlink":null,"image":{},"source":{},"categories":["JavaScript","Node.js","npm","fliptable","spaltenorientiert"],"enclosures":[],"rss:@":{},"rss:title":{"@":{},"#":"Tabellen und JSON"},"rss:description":{"@":{},"#":"<p>Manche Informationen lassen sich einfach am besten in Tabellen darstellen. Soll der Inhalt der Tabelle dynamisch geladen werden, z. B. durch unendliches Scrollen oder einen Weiter-Button, so muss der Inhalt dynamisch nachgeladen werden. Das <a href=\"http://de.wikipedia.org/wiki/JavaScript_Object_Notation\">JSON-Format</a> bietet sich hierbei an, da es auf dem Browser mit <code>JSON.parse(json)</code> in ein JavaScript Wert umgewandelt werden kann.</p>\n\n<h1 id=\"zeilenorientierteransatz\">zeilenorientierter Ansatz</h1>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/zeilenorientiert-1.png\" alt=\"zeilenorientierter Ansatz\">\nEin zeilenorientierter Ansatz wäre die Daten als Liste von Objekten zu übertragen.</p>\n\n<pre><code>[\n   {\"name\": \"Anne\", \"age\": 25, \"city\": \"Stuttgart\"},\n   {\"name\": \"Mike\", \"age\": 33, \"city\": \"London\"}\n   {\"name\": \"Juli\", \"age\": 27, \"city\": \"Paris\"}\n]\n</code></pre>\n\n<p>Wie in den meisten Datenbanken werden einzelne Einträge als Zeile dargestellt und gespeichert. <br>\nAuffallend ist jedoch, dass sehr viele redundante Informationen vorhanden sind, da wir die Spaltennamen (name, age, city) in jeder Zeile wiederholen.</p>\n\n<p>Können wir in Zeiten von mobilen Endgeräten unseren Nutzern nicht etwas Bandbreite sparen?</p>\n\n<h1 id=\"spaltenorientierteransatz\">spaltenorientierter Ansatz</h1>\n\n<p><img src=\"http://blog.michaelwittig.info/content/images/2014/Sep/spaltenorientiert.png\" alt=\"spaltenorientierter Ansatz\"></p>\n\n<p>Ein spaltenorientierter Ansatz wäre die Daten als Objekt von Listen zu übertragen.</p>\n\n<pre><code>{\n \"name\": [\"Anne\", \"Mike\", \"Juli\"],\n \"age\": [25, 33, 27],\n \"city\": [\"Stuttgart\", \"London\", \"Paris\"]\n}\n</code></pre>\n\n<p>Die redundanten Informationen wurden eliminiert. Übrig bleibt die minimale Information die für die Tabelle benötigt wird.</p>\n\n<h1 id=\"fliptable\">fliptable</h1>\n\n<p>Das Node.js Modul <a href=\"https://www.npmjs.org/package/fliptable\">fliptable</a> bietet genau diese Funktionalität. Es wandelt zeilenorientierte Tabellen in spaltenorientierte um (und vice versa).</p>\n\n<p>Installieren kannst du das Modul mit</p>\n\n<pre><code>npm install fliptable\n</code></pre>\n\n<p>eine zeilenorientierte Tabelle in eine spaltenorientierte flippen</p>\n\n<pre><code>var fliptable = require(\"fliptable\");\nfliptable([{a: 1}, {a: 2}, {a: 3}])\n=&gt;\n{a: [1, 2, 3]}\n</code></pre>\n\n<p>Funktioniert auch vice versa</p>\n\n<pre><code>fliptable({a: [1, 2, 3]})\n=&gt;\n[{a: 1}, {a: 2}, {a: 3}]\n</code></pre>\n\n<p>Du kannst also die Tabelle auf dem Server vor dem versenden flippen und auf dem Client flippen und hast genau die Tabelle wie ohne <code>fliptable</code> nur 50% kleiner bei der Übertragen zum Client.</p>\n\n<blockquote>\n  <p>Das Node.js Modul <a href=\"https://www.npmjs.org/package/fliptable\">fliptable</a> wurde beim <a href=\"http://www.meetup.com/stuttgartjs/events/181026072/\">Javascript-Meetup Stuttgart</a> am 02.06.2014 vorgestellt.</p>\n</blockquote>\n\n<h1 id=\"fazitspare50\">Fazit: Spare 50%</h1>\n\n<p>Eine spaltenorientierter JSON String dieser <a href=\"https://github.com/michaelwittig/fliptable/blob/master/test/data.json\">Beispieltabelle</a> ist 449.600 Zeichen lang (<code>JSON.stringify(table)</code>). <br>\nMit <code>fliptable(table)</code> ist der JSON String nur 212.382 Zeichen lang.</p>\n\n<p>Du sparst als mehr als 50% ohne Information zu verlieren!</p>"},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/tabellen-und-json/"},"rss:guid":{"@":{"ispermalink":"false"},"#":"4adaab2e-e91a-48cc-868a-bd1308ccc8d4"},"rss:category":[{"@":{},"#":"JavaScript"},{"@":{},"#":"Node.js"},{"@":{},"#":"npm"},{"@":{},"#":"fliptable"},{"@":{},"#":"spaltenorientiert"}],"dc:creator":{"@":{},"#":"Michael"},"rss:pubdate":{"@":{},"#":"Fri, 19 Sep 2014 20:42:15 GMT"},"meta":{"#ns":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"@":[{"xmlns:dc":"http://purl.org/dc/elements/1.1/"},{"xmlns:content":"http://purl.org/rss/1.0/modules/content/"},{"xmlns:atom":"http://www.w3.org/2005/Atom"}],"#xml":{"version":"1.0","encoding":"UTF-8"},"#type":"rss","#version":"2.0","title":"Michael Wittig","description":"Daten, Programmierung, Web, Finanzen und Bücher.","date":"2015-05-06T12:50:21.000Z","pubdate":"2015-05-06T12:50:21.000Z","pubDate":"2015-05-06T12:50:21.000Z","link":"http://blog.michaelwittig.info/","xmlurl":"http://blog.michaelwittig.info/rss/","xmlUrl":"http://blog.michaelwittig.info/rss/","author":null,"language":null,"favicon":null,"copyright":null,"generator":"Ghost 0.5","cloud":{},"image":{},"categories":[],"rss:@":{},"rss:title":{"@":{},"#":"Michael Wittig"},"rss:description":{"@":{},"#":"Daten, Programmierung, Web, Finanzen und Bücher."},"rss:link":{"@":{},"#":"http://blog.michaelwittig.info/"},"rss:generator":{"@":{},"#":"Ghost 0.5"},"rss:lastbuilddate":{"@":{},"#":"Wed, 06 May 2015 12:50:21 GMT"},"atom:link":{"@":{"href":"http://blog.michaelwittig.info/rss/","rel":"self","type":"application/rss+xml"}},"rss:ttl":{"@":{},"#":"60"}}}]